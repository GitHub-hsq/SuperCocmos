# Langchain å·¥ä½œæµå¼€å‘

ä½¿ç”¨ LangGraph å’Œ ChatOpenAI åˆ›å»º AI é©±åŠ¨çš„å·¥ä½œæµã€‚

---

## ğŸ“ æ–‡ä»¶ä½ç½®

Langchain å·¥ä½œæµä½äºï¼š
```
service/src/
â”œâ”€â”€ quiz/              # æµ‹éªŒç”Ÿæˆå·¥ä½œæµ
â”‚   â”œâ”€â”€ workflow.ts    # ä¸»å·¥ä½œæµå®šä¹‰
â”‚   â”œâ”€â”€ types.ts       # ç±»å‹å®šä¹‰
â”‚   â”œâ”€â”€ defaultPrompts.ts
â”‚   â””â”€â”€ loader.ts
â””â”€â”€ novel/             # å°è¯´ç”Ÿæˆå·¥ä½œæµ
    â”œâ”€â”€ workflows/
    â”œâ”€â”€ prompts/
    â””â”€â”€ types.ts
```

---

## ğŸ¯ å·¥ä½œæµæ ¸å¿ƒæ¦‚å¿µ

### StateGraph
LangGraph ä½¿ç”¨çŠ¶æ€å›¾ï¼ˆStateGraphï¼‰å®šä¹‰å·¥ä½œæµï¼š

```typescript
import { StateGraph, END } from '@langchain/langgraph'

const workflow = new StateGraph<MyWorkflowState>({
  channels: myStateChannels
})

workflow.addNode('step1', step1Function)
workflow.addNode('step2', step2Function)

workflow.addEdge('step1', 'step2')
workflow.addEdge('step2', END)

const app = workflow.compile()
```

### èŠ‚ç‚¹ï¼ˆNodesï¼‰
æ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ï¼Œå¤„ç†çŠ¶æ€å¹¶è¿”å›æ›´æ–°ï¼š

```typescript
async function myNode(state: MyWorkflowState): Promise<Partial<MyWorkflowState>> {
  // å¤„ç†é€»è¾‘
  const result = await someOperation()

  // è¿”å›çŠ¶æ€æ›´æ–°
  return {
    someField: result,
    anotherField: state.anotherField + 1
  }
}
```

---

## ğŸ“ åŸºæœ¬å·¥ä½œæµæ¨¡æ¿

```typescript
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { StateGraph, END } from '@langchain/langgraph'
import { ChatOpenAI } from '@langchain/openai'

/**
 * å·¥ä½œæµçŠ¶æ€æ¥å£
 */
interface MyWorkflowState {
  input: string
  step1Result?: string
  step2Result?: string
  finalOutput?: string
  errors: string[]
}

/**
 * åˆ›å»º LLM å®ä¾‹
 */
function createLLM(apiKey: string, model: string = 'gpt-4o-mini') {
  return new ChatOpenAI({
    model,
    apiKey,
    temperature: 0,
    maxTokens: 2000,
  })
}

/**
 * ç¬¬ä¸€æ­¥ï¼šåˆ†æè¾“å…¥
 */
async function analyzeInput(state: MyWorkflowState): Promise<Partial<MyWorkflowState>> {
  try {
    const llm = createLLM(process.env.OPENAI_API_KEY!)

    const prompt = ChatPromptTemplate.fromMessages([
      ['system', 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ†æåŠ©æ‰‹'],
      ['human', 'è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š{input}']
    ])

    const chain = prompt.pipe(llm)
    const result = await chain.invoke({ input: state.input })

    return {
      step1Result: result.content as string
    }
  } catch (error) {
    console.error('âŒ [analyzeInput] é”™è¯¯:', error)
    return {
      errors: [...state.errors, `åˆ†æå¤±è´¥: ${error}`]
    }
  }
}

/**
 * ç¬¬äºŒæ­¥ï¼šå¤„ç†ç»“æœ
 */
async function processResult(state: MyWorkflowState): Promise<Partial<MyWorkflowState>> {
  try {
    const llm = createLLM(process.env.OPENAI_API_KEY!)

    const prompt = ChatPromptTemplate.fromMessages([
      ['system', 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤„ç†åŠ©æ‰‹'],
      ['human', 'è¯·å¤„ç†ä»¥ä¸‹åˆ†æç»“æœï¼š{analysis}']
    ])

    const chain = prompt.pipe(llm)
    const result = await chain.invoke({ analysis: state.step1Result })

    return {
      step2Result: result.content as string,
      finalOutput: result.content as string
    }
  } catch (error) {
    console.error('âŒ [processResult] é”™è¯¯:', error)
    return {
      errors: [...state.errors, `å¤„ç†å¤±è´¥: ${error}`]
    }
  }
}

/**
 * æ„å»ºå¹¶è¿è¡Œå·¥ä½œæµ
 */
export async function runMyWorkflow(input: string) {
  // å®šä¹‰çŠ¶æ€é€šé“
  const stateChannels = {
    input: null,
    step1Result: null,
    step2Result: null,
    finalOutput: null,
    errors: null,
  }

  // åˆ›å»ºå·¥ä½œæµ
  const workflow = new StateGraph<MyWorkflowState>({
    channels: stateChannels
  })

  // æ·»åŠ èŠ‚ç‚¹
  workflow.addNode('analyze', analyzeInput)
  workflow.addNode('process', processResult)

  // å®šä¹‰æµç¨‹
  workflow.setEntryPoint('analyze')
  workflow.addEdge('analyze', 'process')
  workflow.addEdge('process', END)

  // ç¼–è¯‘å·¥ä½œæµ
  const app = workflow.compile()

  // æ‰§è¡Œå·¥ä½œæµ
  const initialState: MyWorkflowState = {
    input,
    errors: []
  }

  const finalState = await app.invoke(initialState)
  return finalState
}
```

---

## ğŸš€ å®é™…é¡¹ç›®ç¤ºä¾‹

### Quiz Workflowï¼ˆæµ‹éªŒç”Ÿæˆï¼‰

```typescript
/**
 * å·¥ä½œæµçŠ¶æ€
 */
interface WorkflowState {
  file_content: string
  file_path: string
  classification_label: ClassificationLabel
  subject: Subject
  extracted_paragraphs: string[]
  raw_questions: string
  questions: QuizItem[]
  human_feedback: HumanFeedbackInput | null
  review_result: string
  node_configs: WorkflowNodeConfig[]
  model_info?: ModelInfo
}

/**
 * åˆ†ç±»èŠ‚ç‚¹ï¼šåˆ¤æ–­æ–‡æ¡£ç±»å‹
 */
async function classifier(state: WorkflowState): Promise<Partial<WorkflowState>> {
  const nodeConfig = state.node_configs.find(n => n.type === 'classifier')
  const llm = makeLLM(state.model_info, nodeConfig?.model_config)

  const prompt = ChatPromptTemplate.fromMessages([
    ['system', DEFAULT_CLASSIFIER_PROMPT],
    ['human', '{text}']
  ])

  const chain = prompt.pipe(llm)
  const result = await chain.invoke({
    text: state.file_content.slice(0, 2000)
  })

  const content = result.content as string
  const label = parseClassificationLabel(content)

  console.log('ğŸ“Œ åˆ†ç±»ç»“æœ:', label)

  return {
    classification_label: label
  }
}

/**
 * ç”Ÿæˆé¢˜ç›®èŠ‚ç‚¹
 */
async function generateQuestions(state: WorkflowState): Promise<Partial<WorkflowState>> {
  const nodeConfig = state.node_configs.find(n => n.type === 'generate_questions')
  const llm = makeLLM(state.model_info, nodeConfig?.model_config)

  const prompt = ChatPromptTemplate.fromMessages([
    ['system', DEFAULT_GENERATE_QUESTIONS_PROMPT],
    ['human', '{material}']
  ])

  const chain = prompt.pipe(llm)
  const result = await chain.invoke({
    material: state.extracted_paragraphs.join('\n\n')
  })

  return {
    raw_questions: result.content as string
  }
}

/**
 * æ„å»ºå·¥ä½œæµ
 */
export async function runWorkflow(
  filePath: string,
  workflowNodeConfigs: WorkflowNodeConfig[],
  modelInfo?: ModelInfo
): Promise<WorkflowState> {
  // åŠ è½½æ–‡ä»¶å†…å®¹
  const fileContent = await loadFile(filePath)

  // å®šä¹‰çŠ¶æ€é€šé“
  const stateChannels = {
    file_content: null,
    file_path: null,
    classification_label: null,
    subject: null,
    extracted_paragraphs: null,
    raw_questions: null,
    questions: null,
    human_feedback: null,
    review_result: null,
    node_configs: null,
    model_info: null,
  }

  // åˆ›å»ºå·¥ä½œæµ
  const workflow = new StateGraph<WorkflowState>({
    channels: stateChannels
  })

  // æ·»åŠ èŠ‚ç‚¹
  workflow.addNode('classifier', classifier)
  workflow.addNode('generate_questions', generateQuestions)
  workflow.addNode('parse_questions', parseQuestions)
  workflow.addNode('review_and_score', reviewAndScore)

  // å®šä¹‰æµç¨‹
  workflow.setEntryPoint('classifier')
  workflow.addEdge('classifier', 'generate_questions')
  workflow.addEdge('generate_questions', 'parse_questions')
  workflow.addEdge('parse_questions', 'review_and_score')
  workflow.addEdge('review_and_score', END)

  // ç¼–è¯‘å¹¶æ‰§è¡Œ
  const app = workflow.compile()

  const initialState: WorkflowState = {
    file_content: fileContent,
    file_path: filePath,
    classification_label: 'unknown',
    subject: 'general',
    extracted_paragraphs: [],
    raw_questions: '',
    questions: [],
    human_feedback: null,
    review_result: '',
    node_configs: workflowNodeConfigs,
    model_info: modelInfo,
  }

  const finalState = await app.invoke(initialState)
  return finalState
}
```

---

## ğŸ”§ LLM é…ç½®

### æ”¯æŒå¤šä¸ª AI ä¾›åº”å•†

```typescript
/**
 * åˆ¤æ–­æ¨¡å‹æ˜¯å¦ä¸ºç‰¹å®šä¾›åº”å•†
 */
function isKrioraModel(modelId: string): boolean {
  return modelId.includes('moonshotai/') || modelId.includes('qwen/')
}

/**
 * åˆ›å»º LLM å®ä¾‹ï¼ˆæ”¯æŒå¤šä¾›åº”å•†ï¼‰
 */
function makeLLM(modelInfo?: ModelInfo, config?: ModelConfig) {
  const model = modelInfo?.name || process.env.OPENAI_API_MODEL || 'gpt-4o-mini'

  let apiKey = modelInfo?.apiKey
  let baseURL = modelInfo?.baseURL

  // æ ¹æ®æ¨¡å‹é€‰æ‹© API é…ç½®
  if (!apiKey || !baseURL) {
    if (isKrioraModel(model)) {
      apiKey = apiKey || process.env.KRIORA_API_KEY || process.env.OPENAI_API_KEY
      baseURL = baseURL || process.env.KRIORA_API_URL || 'https://api.kriora.com'
    } else {
      apiKey = apiKey || process.env.OPENAI_API_KEY
      baseURL = baseURL || process.env.OPENAI_API_BASE_URL
    }
  }

  if (!apiKey) {
    throw new Error('API_KEY æœªé…ç½®ï¼')
  }

  return new ChatOpenAI({
    model,
    apiKey,
    configuration: baseURL ? { baseURL } : undefined,
    temperature: config?.temperature ?? 0,
    topP: config?.top_p,
    maxTokens: config?.max_tokens,
    presencePenalty: config?.presence_penalty,
    frequencyPenalty: config?.frequency_penalty,
  })
}
```

### æ¨¡å‹é…ç½®æ¥å£

```typescript
export interface ModelInfo {
  name: string
  provider: string
  apiKey?: string
  baseURL?: string
}

export interface ModelConfig {
  temperature?: number
  top_p?: number
  max_tokens?: number
  presence_penalty?: number
  frequency_penalty?: number
}
```

---

## ğŸ“Š æ¡ä»¶åˆ†æ”¯

ä½¿ç”¨æ¡ä»¶è¾¹å®ç°åˆ†æ”¯é€»è¾‘ï¼š

```typescript
/**
 * è·¯ç”±å‡½æ•°ï¼šæ ¹æ®åˆ†ç±»ç»“æœå†³å®šä¸‹ä¸€æ­¥
 */
function routeByClassification(state: WorkflowState): string {
  switch (state.classification_label) {
    case 'textbook':
      return 'extract_textbook'
    case 'article':
      return 'extract_article'
    case 'unknown':
    default:
      return 'extract_general'
  }
}

// åœ¨å·¥ä½œæµä¸­ä½¿ç”¨
workflow.addConditionalEdges(
  'classifier',
  routeByClassification,
  {
    'extract_textbook': 'textbook_extractor',
    'extract_article': 'article_extractor',
    'extract_general': 'general_extractor',
  }
)
```

---

## âš ï¸ é”™è¯¯å¤„ç†

### èŠ‚ç‚¹çº§åˆ«é”™è¯¯å¤„ç†

```typescript
async function myNode(state: MyWorkflowState): Promise<Partial<MyWorkflowState>> {
  try {
    const llm = createLLM(process.env.OPENAI_API_KEY!)
    const result = await llm.invoke('prompt')

    return {
      result: result.content as string
    }
  } catch (error) {
    console.error('âŒ [myNode] é”™è¯¯:', error)

    // å°†é”™è¯¯è®°å½•åˆ°çŠ¶æ€ä¸­
    return {
      errors: [...state.errors, `èŠ‚ç‚¹å¤±è´¥: ${error}`],
      result: '' // æä¾›é»˜è®¤å€¼
    }
  }
}
```

### å·¥ä½œæµçº§åˆ«é”™è¯¯å¤„ç†

```typescript
export async function runWorkflow(input: string) {
  try {
    const app = workflow.compile()
    const result = await app.invoke(initialState)

    // æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if (result.errors.length > 0) {
      console.error('âš ï¸  å·¥ä½œæµå®Œæˆä½†æœ‰é”™è¯¯:', result.errors)
    }

    return result
  } catch (error) {
    console.error('âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥:', error)
    throw error
  }
}
```

---

## ğŸ“ Prompt æœ€ä½³å®è·µ

### ä½¿ç”¨ ChatPromptTemplate

```typescript
import { ChatPromptTemplate } from '@langchain/core/prompts'

// âœ… æ¨èï¼šä½¿ç”¨ç»“æ„åŒ– prompt
const prompt = ChatPromptTemplate.fromMessages([
  ['system', 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}'],
  ['human', 'è¯·{action}ï¼š{content}']
])

const chain = prompt.pipe(llm)
const result = await chain.invoke({
  role: 'ç¼–è¾‘',
  action: 'æ¶¦è‰²',
  content: 'åŸæ–‡å†…å®¹...'
})

// âŒ é¿å…ï¼šç¡¬ç¼–ç å­—ç¬¦ä¸²
const result = await llm.invoke(`ä½ æ˜¯ç¼–è¾‘ï¼Œè¯·æ¶¦è‰²ï¼š${content}`)
```

### åˆ†ç¦» Prompt å®šä¹‰

```typescript
// defaultPrompts.ts
export const CLASSIFIER_PROMPT = `ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†ç±»ä¸“å®¶...`

export const GENERATE_QUESTIONS_PROMPT = `ä½ æ˜¯ä¸€ä¸ªé¢˜ç›®ç”Ÿæˆä¸“å®¶...`

// workflow.ts
import { CLASSIFIER_PROMPT } from './defaultPrompts'

const prompt = ChatPromptTemplate.fromMessages([
  ['system', CLASSIFIER_PROMPT],
  ['human', '{text}']
])
```

---

## ğŸ“‹ æ£€æŸ¥æ¸…å•

åˆ›å»º Langchain å·¥ä½œæµæ—¶ï¼Œç¡®è®¤ï¼š

- [ ] å®šä¹‰æ¸…æ™°çš„ State æ¥å£
- [ ] æ‰€æœ‰èŠ‚ç‚¹å‡½æ•°éƒ½æ˜¯ async
- [ ] èŠ‚ç‚¹è¿”å› Partial<State>
- [ ] ä½¿ç”¨ try-catch å¤„ç†é”™è¯¯
- [ ] LLM é…ç½®æ”¯æŒå¤šä¾›åº”å•†
- [ ] Prompt ä½¿ç”¨ ChatPromptTemplate
- [ ] è®°å½•å…³é”®æ­¥éª¤æ—¥å¿—
- [ ] å·¥ä½œæµæœ‰æ˜ç¡®çš„ END ç‚¹
- [ ] æµ‹è¯•ä¸åŒçš„è¾“å…¥åœºæ™¯
- [ ] å¤„ç† API è¶…æ—¶å’Œé™æµ

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. çŠ¶æ€è®¾è®¡
```typescript
// âœ… æ¨èï¼šç»“æ„åŒ–çŠ¶æ€
interface WorkflowState {
  input: string
  step1Result?: string
  step2Result?: string
  errors: string[]
  metadata: {
    startTime: number
    duration?: number
  }
}

// âŒ é¿å…ï¼šæ‰å¹³æ— ç±»å‹
interface WorkflowState {
  data: any
  result: any
}
```

### 2. æ—¥å¿—è®°å½•
```typescript
async function myNode(state: WorkflowState): Promise<Partial<WorkflowState>> {
  console.log('ğŸš€ [myNode] å¼€å§‹æ‰§è¡Œ')
  const start = performance.now()

  try {
    const result = await processData()
    const duration = performance.now() - start

    console.log(`âœ… [myNode] å®Œæˆï¼Œè€—æ—¶: ${duration.toFixed(0)}ms`)
    return { result }
  } catch (error) {
    const duration = performance.now() - start
    console.error(`âŒ [myNode] å¤±è´¥ï¼Œè€—æ—¶: ${duration.toFixed(0)}ms`, error)
    return { errors: [...state.errors, error] }
  }
}
```

### 3. å¯é…ç½®èŠ‚ç‚¹
```typescript
interface NodeConfig {
  type: WorkflowNodeType
  enabled: boolean
  model_config?: ModelConfig
}

async function myNode(state: WorkflowState): Promise<Partial<WorkflowState>> {
  const nodeConfig = state.node_configs.find(n => n.type === 'my_node')

  if (!nodeConfig?.enabled) {
    console.log('â­ï¸  [myNode] å·²ç¦ç”¨ï¼Œè·³è¿‡')
    return {}
  }

  const llm = makeLLM(state.model_info, nodeConfig.model_config)
  // ...
}
```

---

## ğŸ”— ç›¸å…³èµ„æº

- [LangGraph å®˜æ–¹æ–‡æ¡£](https://js.langchain.com/docs/langgraph)
- [ChatOpenAI é…ç½®](https://js.langchain.com/docs/integrations/chat/openai)
- [Prompt æ¨¡æ¿](https://js.langchain.com/docs/modules/model_io/prompts/quick_start)

---

**è®°ä½**ï¼šLangchain å·¥ä½œæµåº”è¯¥æ˜¯å¯è§‚æµ‹ã€å¯é…ç½®å’Œå®¹é”™çš„ã€‚å……åˆ†åˆ©ç”¨ TypeScript çš„ç±»å‹ç³»ç»Ÿç¡®ä¿çŠ¶æ€çš„æ­£ç¡®ä¼ é€’ã€‚
