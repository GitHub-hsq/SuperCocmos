import type { ChatGPTAPIOptions, ChatMessage, SendMessageOptions } from 'chatgpt'
import type { ApiModel, ChatContext, ChatGPTUnofficialProxyAPIOptions, ModelConfig } from '../types'
import type { RequestOptions, SetProxyOptions, UsageResponse } from './types'
import { ChatGPTAPI, ChatGPTUnofficialProxyAPI } from 'chatgpt'
import * as dotenv from 'dotenv'
import httpsProxyAgent from 'https-proxy-agent'
import fetch from 'node-fetch'
import { SocksProxyAgent } from 'socks-proxy-agent'
import { getProviderById } from '../db/providerService' // ğŸ”¥ å¯¼å…¥ä¾›åº”å•†æœåŠ¡
import { sendResponse } from '../utils'
import { isNotEmptyString } from '../utils/is'
import 'isomorphic-fetch'

const { HttpsProxyAgent } = httpsProxyAgent

dotenv.config()

const ErrorCodeMessage: Record<string, string> = {
  401: '[OpenAI] æä¾›é”™è¯¯çš„APIå¯†é’¥ | Incorrect API key provided',
  403: '[OpenAI] æœåŠ¡å™¨æ‹’ç»è®¿é—®ï¼Œè¯·ç¨åå†è¯• | Server refused to access, please try again later',
  502: '[OpenAI] é”™è¯¯çš„ç½‘å…³ |  Bad Gateway',
  503: '[OpenAI] æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åå†è¯• | Server is busy, please try again later',
  504: '[OpenAI] ç½‘å…³è¶…æ—¶ | Gateway Time-out',
  500: '[OpenAI] æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åå†è¯• | Internal Server Error',
}

const timeoutMs: number = !Number.isNaN(+process.env.TIMEOUT_MS) ? +process.env.TIMEOUT_MS : 100 * 1000
const disableDebug: boolean = process.env.OPENAI_API_DISABLE_DEBUG === 'true'

// æŠ‘åˆ¶ chatgpt åº“çš„ token è®¡ç®—é”™è¯¯æ—¥å¿—
// è¿™äº›é”™è¯¯é€šå¸¸æ˜¯ç”±äºç½‘ç»œé—®é¢˜å¯¼è‡´çš„ tiktoken æ¨¡å‹ä¸‹è½½å¤±è´¥
const originalConsoleWarn = console.warn
const originalConsoleError = console.error

// è®°å½•æœ€è¿‘çš„é”™è¯¯æ¶ˆæ¯ï¼Œé˜²æ­¢é‡å¤æ‰“å°
const recentErrors = new Set<string>()
const ERROR_CACHE_TIME = 5000 // 5ç§’å†…çš„é‡å¤é”™è¯¯ä¸æ˜¾ç¤º

console.warn = (...args: any[]) => {
  const msg = String(args[0] || '')
  // è¿‡æ»¤æ‰ token è®¡ç®—ç›¸å…³çš„è­¦å‘Š
  if (msg.includes('Failed to calculate number of tokens')
    || msg.includes('falling back to approximate count')) {
    return
  }

  originalConsoleWarn.apply(console, args)
}

console.error = (...args: any[]) => {
  const msg = String(args[0] || '')

  // è¿‡æ»¤æ‰ token è®¡ç®—ç›¸å…³çš„ ECONNRESET é”™è¯¯
  if (msg.includes('Failed to calculate number of tokens'))
    return

  // é˜²æ­¢çŸ­æ—¶é—´å†…é‡å¤æ‰“å°ç›¸åŒçš„é”™è¯¯
  const errorKey = msg.substring(0, 100)
  if (recentErrors.has(errorKey))
    return

  recentErrors.add(errorKey)
  setTimeout(() => recentErrors.delete(errorKey), ERROR_CACHE_TIME)

  originalConsoleError.apply(console, args)
}

let apiModel: ApiModel
let api: ChatGPTAPI | ChatGPTUnofficialProxyAPI
let isInitialized = false

// å»¶è¿Ÿåˆå§‹åŒ–å‡½æ•°
async function initializeAPI() {
  if (isInitialized)
    return

  const model = isNotEmptyString(process.env.OPENAI_API_MODEL) ? process.env.OPENAI_API_MODEL : 'gpt-3.5-turbo'

  // åªæœ‰åœ¨å®é™…éœ€è¦æ—¶æ‰æ£€æŸ¥ç¯å¢ƒå˜é‡
  if (!isNotEmptyString(process.env.OPENAI_API_KEY) && !isNotEmptyString(process.env.OPENAI_ACCESS_TOKEN)) {
    console.warn('âš ï¸ [ChatGPT] æœªé…ç½® OPENAI_API_KEY æˆ– OPENAI_ACCESS_TOKENï¼Œå°†ä½¿ç”¨æ•°æ®åº“ä¸­çš„ä¾›åº”å•†é…ç½®')
    isInitialized = true
    return
  }

  await (async () => {
  // More Info: https://github.com/transitive-bullshit/chatgpt-api

    if (isNotEmptyString(process.env.OPENAI_API_KEY)) {
      const OPENAI_API_BASE_URL = process.env.OPENAI_API_BASE_URL

      const options: ChatGPTAPIOptions = {
        apiKey: process.env.OPENAI_API_KEY,
        completionParams: { model },
        debug: !disableDebug,
        // ç¦ç”¨ token è®¡æ•°ä»¥é¿å…ç½‘ç»œé”™è¯¯
        // chatgptåº“ä¼šå°è¯•ä»ç½‘ç»œä¸‹è½½tiktokenæ¨¡å‹ï¼Œå¯èƒ½å¯¼è‡´ECONNRESETé”™è¯¯
        messageStore: undefined,
      }

      // increase max token limit if use gpt-4
      if (model.toLowerCase().includes('gpt-4')) {
      // if use 32k model
        if (model.toLowerCase().includes('32k')) {
          options.maxModelTokens = 32768
          options.maxResponseTokens = 8192
        }
        else if (/-4o-mini/.test(model.toLowerCase())) {
          options.maxModelTokens = 128000
          options.maxResponseTokens = 16384
        }
        // if use GPT-4 Turbo or GPT-4o
        else if (/-preview|-turbo|o/.test(model.toLowerCase())) {
          options.maxModelTokens = 128000
          options.maxResponseTokens = 4096
        }
        else {
          options.maxModelTokens = 8192
          options.maxResponseTokens = 2048
        }
      }
      else if (model.toLowerCase().includes('gpt-3.5')) {
        if (/16k|1106|0125/.test(model.toLowerCase())) {
          options.maxModelTokens = 16384
          options.maxResponseTokens = 4096
        }
      }

      if (isNotEmptyString(OPENAI_API_BASE_URL)) {
      // æ¨¡å‹è°ƒç”¨éœ€è¦åŠ  /v1
        options.apiBaseUrl = `${OPENAI_API_BASE_URL}/v1`
      }

      setupProxy(options as any)

      api = new ChatGPTAPI({ ...options })
      apiModel = 'ChatGPTAPI'
    }
    else {
      const options: ChatGPTUnofficialProxyAPIOptions = {
        accessToken: process.env.OPENAI_ACCESS_TOKEN,
        apiReverseProxyUrl: isNotEmptyString(process.env.API_REVERSE_PROXY) ? process.env.API_REVERSE_PROXY : 'https://ai.fakeopen.com/api/conversation',
        model,
        debug: !disableDebug,
      }

      setupProxy(options as any)

      api = new ChatGPTUnofficialProxyAPI({ ...options })
      apiModel = 'ChatGPTUnofficialProxyAPI'
    }
  })()

  isInitialized = true
}

// åˆ¤æ–­æ¨¡å‹æ˜¯å¦ä¸º Kriora ä¾›åº”å•†
function isKrioraModel(modelId: string): boolean {
  return modelId.includes('moonshotai/') || modelId.includes('qwen/')
}

// ä¸ºç‰¹å®šä¾›åº”å•†åˆ›å»º API å®ä¾‹
function createApiForProvider(modelId: string, maxTokens?: number): ChatGPTAPI {
  if (isKrioraModel(modelId)) {
    // ä½¿ç”¨ Kriora API é…ç½®
    const krioraApiKey = process.env.KRIORA_API_KEY || process.env.OPENAI_API_KEY
    const krioraApiUrl = process.env.KRIORA_API_URL || 'https://api.kriora.com'

    const options: ChatGPTAPIOptions = {
      apiKey: krioraApiKey,
      completionParams: { model: modelId },
      debug: !disableDebug,
      messageStore: undefined,
      apiBaseUrl: `${krioraApiUrl}/v1`,
      maxModelTokens: 128000,
      maxResponseTokens: maxTokens || 8192, // ä½¿ç”¨é…ç½®çš„ maxTokensï¼Œé»˜è®¤ 8192
    }

    setupProxy(options as any)
    return new ChatGPTAPI({ ...options })
  }

  // é»˜è®¤ä½¿ç”¨å…¨å±€ API å®ä¾‹ï¼ˆå¯èƒ½æœªåˆå§‹åŒ–ï¼‰
  return api as ChatGPTAPI
}

// å¯¹è¯å†å²å­˜å‚¨ï¼ˆç®€å•çš„å†…å­˜å­˜å‚¨ï¼Œç”Ÿäº§ç¯å¢ƒåº”è¯¥ç”¨æ•°æ®åº“æˆ– Redisï¼‰
const conversationHistory = new Map<string, Array<{ role: string, content: string }>>()

/**
 * ğŸš€ åŸç”Ÿå®ç°çš„èŠå¤©å›å¤å¤„ç†ï¼ˆæ›´å¿«ã€æ›´å¯æ§ï¼‰
 * ç›´æ¥ä½¿ç”¨ fetch API è°ƒç”¨ OpenAI å…¼å®¹çš„æ¥å£
 * å·²ç¦ç”¨ï¼Œä½¿ç”¨ chatgpt åº“ä»£æ›¿
 */
async function _chatReplyProcessNative(options: RequestOptions) {
  const { message, lastContext, process: processCallback, systemMessage, temperature, top_p, model: requestModel, maxTokens, baseURL, apiKey } = options

  if (!baseURL || !apiKey)
    throw new Error('ç¼ºå°‘å¿…éœ€çš„å‚æ•°: baseURL æˆ– apiKey')

  try {
    const apiUrl = baseURL.endsWith('/v1')
      ? `${baseURL}/chat/completions`
      : `${baseURL}/v1/chat/completions`

    // ğŸ”¥ æ„å»ºå¯¹è¯å†å²
    const conversationId = lastContext?.conversationId || `conv_${Date.now()}`
    let messages: Array<{ role: string, content: string }> = []

    // å¦‚æœæœ‰å†å²è®°å½•ï¼ŒåŠ è½½å®ƒ
    if (conversationHistory.has(conversationId)) {
      messages = conversationHistory.get(conversationId)!
      console.warn(`ğŸ“š [åŸç”Ÿå®ç°] åŠ è½½å†å²å¯¹è¯: ${messages.length} æ¡æ¶ˆæ¯`)
    }
    else {
      // æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
      if (systemMessage)
        messages.push({ role: 'system', content: systemMessage })
    }

    // æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    messages.push({ role: 'user', content: message })

    const requestBody = {
      model: requestModel || 'gpt-3.5-turbo',
      messages,
      temperature: temperature || 0.7,
      top_p: top_p || 1,
      max_tokens: maxTokens || 4096,
      stream: true, // ä½¿ç”¨æµå¼å“åº”
    }

    console.warn('ğŸš€ [åŸç”Ÿå®ç°] å‘é€è¯·æ±‚:', {
      url: apiUrl,
      model: requestBody.model,
      messageLength: message.length,
    })

    const startTime = Date.now()
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestBody),
      signal: AbortSignal.timeout(100000), // 100ç§’è¶…æ—¶
    })

    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`API è¯·æ±‚å¤±è´¥: ${response.status} ${errorText}`)
    }

    // å¤„ç†æµå¼å“åº”ï¼ˆå…¼å®¹ Node.js Readable Streamï¼‰
    let fullText = ''
    let reasoningText = ''
    const messageId = `msg_${Date.now()}`
    let chunkCount = 0
    let lastLogTime = Date.now()
    let hasReceivedContent = false

    if (response.body) {
      // Node.js ç¯å¢ƒä¸­ response.body æ˜¯ Readable stream
      let buffer = ''

      // ç›‘å¬ data äº‹ä»¶
      for await (const chunk of response.body as any) {
        chunkCount++
        buffer += chunk.toString()

        // æŒ‰è¡Œå¤„ç†æ•°æ®
        const lines = buffer.split('\n')
        // ä¿ç•™æœ€åä¸€ä¸ªä¸å®Œæ•´çš„è¡Œ
        buffer = lines.pop() || ''

        for (const line of lines) {
          const trimmedLine = line.trim()
          if (!trimmedLine)
            continue

          if (trimmedLine.startsWith('data: ')) {
            const data = trimmedLine.slice(6)
            if (data === '[DONE]')
              continue

            try {
              const parsed = JSON.parse(data)
              const choice = parsed.choices?.[0]

              // ğŸ”¥ åŒæ—¶å¤„ç† content å’Œ reasoning_content
              const contentDelta = choice?.delta?.content || ''
              const reasoningDelta = choice?.delta?.reasoning_content || ''

              // ğŸ”¥ å¤„ç†æ€è€ƒè¿‡ç¨‹ï¼ˆæ˜¾ç¤ºç»™ç”¨æˆ·ï¼Œè®©ä»–ä»¬çŸ¥é“ AI åœ¨æ€è€ƒï¼‰
              if (reasoningDelta && !hasReceivedContent) {
                reasoningText += reasoningDelta
                // å‘é€æ€è€ƒçŠ¶æ€ç»™å‰ç«¯ï¼ˆä½¿ç”¨ç‰¹æ®Šæ ¼å¼ï¼Œå‰ç«¯å¯ä»¥è¯†åˆ«ï¼‰
                processCallback?.({
                  id: messageId,
                  text: `ğŸ’­ 
                  ...\n${reasoningText.substring(0, 100)}...`,
                  role: 'assistant',
                  conversationId,
                  parentMessageId: messageId,
                })
              }

              // ğŸ”¥ å¦‚æœæœ‰å®é™…å†…å®¹ï¼Œå‘é€ç»™å‰ç«¯
              if (contentDelta) {
                hasReceivedContent = true
                fullText += contentDelta
                processCallback?.({
                  id: messageId,
                  text: fullText,
                  role: 'assistant',
                  conversationId,
                  parentMessageId: messageId,
                })
              }

              // è®°å½•ç¬¬ä¸€ä¸ª chunk çš„å®Œæ•´å†…å®¹ï¼Œç”¨äºè°ƒè¯•
              if (chunkCount === 1) {
                console.warn('ğŸ” [åŸç”Ÿå®ç°] ç¬¬ä¸€ä¸ª chunk ç¤ºä¾‹:', JSON.stringify(parsed).substring(0, 200))
              }
            }
            catch {
              // å¿½ç•¥è§£æé”™è¯¯
            }
          }
        }

        // æ¯5ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
        const now = Date.now()
        if (now - lastLogTime > 5000) {
          console.warn(`â±ï¸ [åŸç”Ÿå®ç°] å·²æ¥æ”¶ ${chunkCount} ä¸ª chunksï¼Œæ€è€ƒ ${reasoningText.length} å­—ç¬¦ï¼Œå›å¤ ${fullText.length} å­—ç¬¦ï¼Œè€—æ—¶ ${now - startTime}ms`)
          lastLogTime = now
        }
      }

      console.warn(`ğŸ“Š [åŸç”Ÿå®ç°] æ€»å…±æ¥æ”¶ ${chunkCount} ä¸ª chunksï¼Œæ€è€ƒ ${reasoningText.length} å­—ç¬¦ï¼Œå›å¤ ${fullText.length} å­—ç¬¦`)
    }

    const endTime = Date.now()
    console.warn(`âœ… [åŸç”Ÿå®ç°] å®Œæˆï¼Œè€—æ—¶: ${endTime - startTime}ms`)

    // ğŸ”¥ ä¿å­˜åŠ©æ‰‹å›å¤åˆ°å¯¹è¯å†å²
    messages.push({ role: 'assistant', content: fullText })
    conversationHistory.set(conversationId, messages)

    // ğŸ”¥ é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼ˆåªä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯ï¼‰
    if (messages.length > 20) {
      // ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ + æœ€è¿‘10è½®å¯¹è¯ï¼ˆ20æ¡æ¶ˆæ¯ï¼‰
      const systemMsg = messages[0].role === 'system' ? [messages[0]] : []
      conversationHistory.set(conversationId, [...systemMsg, ...messages.slice(-20)])
      console.warn(`ğŸ—‘ï¸ [åŸç”Ÿå®ç°] å†å²è®°å½•è¿‡é•¿ï¼Œå·²æ¸…ç†æ—§æ¶ˆæ¯`)
    }

    console.warn(`ğŸ’¾ [åŸç”Ÿå®ç°] å·²ä¿å­˜å¯¹è¯å†å²ï¼Œæ€»è®¡ ${messages.length} æ¡æ¶ˆæ¯`)

    return sendResponse({
      type: 'Success',
      data: {
        id: messageId,
        text: fullText,
        role: 'assistant',
        conversationId,
        parentMessageId: messageId,
      },
    })
  }
  catch (error: any) {
    console.error('âŒ [åŸç”Ÿå®ç°] å¤±è´¥:', error)
    return sendResponse({ type: 'Fail', message: error.message || 'è¯·æ±‚å¤±è´¥' })
  }
}

async function chatReplyProcess(options: RequestOptions) {
  // ğŸ”¥ é»˜è®¤ä½¿ç”¨ chatgpt åº“
  console.warn('ğŸ“š [ChatGPT] ä½¿ç”¨ chatgpt åº“')

  // ç¡®ä¿APIå·²åˆå§‹åŒ–
  await initializeAPI()

  const { message, lastContext, process: processCallback, systemMessage, temperature, top_p, model: requestModel, maxTokens, providerId, baseURL, apiKey } = options
  try {
    let options: SendMessageOptions = { timeoutMs }
    const defaultModel = isNotEmptyString(process.env.OPENAI_API_MODEL) ? process.env.OPENAI_API_MODEL : 'gpt-3.5-turbo'
    const selectedModel = requestModel || defaultModel

    // ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ç›´æ¥ä¼ é€’çš„ baseURL å’Œ apiKeyï¼ˆæ–°æ–¹å¼ï¼‰
    let apiInstance: ChatGPTAPI | ChatGPTUnofficialProxyAPI | null = api
    let providerInfo: { baseUrl: string, apiKey: string, name: string } | null = null

    if (baseURL && apiKey) {
      // ğŸ”¥ æ–°æ–¹å¼ï¼šç›´æ¥ä½¿ç”¨ä¼ é€’çš„é…ç½®
      providerInfo = {
        baseUrl: baseURL,
        apiKey,
        name: 'Direct Config',
      }
      console.warn('âœ… [ChatGPT] ä½¿ç”¨ç›´æ¥ä¼ é€’çš„é…ç½®:', {
        baseUrl: baseURL,
        model: selectedModel,
      })

      // ğŸ”¥ åˆ›å»º API å®ä¾‹ï¼ˆä¸ä¾èµ– apiModelï¼Œç›´æ¥ä½¿ç”¨ ChatGPTAPIï¼‰
      // ChatGPT API éœ€è¦å®Œæ•´çš„ URLï¼ŒåŒ…æ‹¬ /v1
      const apiBaseUrl = providerInfo.baseUrl.endsWith('/v1')
        ? providerInfo.baseUrl
        : `${providerInfo.baseUrl}/v1`

      const providerOptions: ChatGPTAPIOptions = {
        apiKey: providerInfo.apiKey,
        completionParams: { model: selectedModel },
        debug: !disableDebug,
        messageStore: undefined,
        apiBaseUrl,
        maxModelTokens: 128000,
        maxResponseTokens: maxTokens || 8192,
      }

      setupProxy(providerOptions as any)
      apiInstance = new ChatGPTAPI({ ...providerOptions })
      console.warn('ğŸ”§ [ChatGPT] å·²åˆ›å»º API å®ä¾‹ï¼ŒURL:', apiBaseUrl)
    }
    else if (lastContext?.providerId || providerId) {
      // ğŸ”¥ æ—§æ–¹å¼ï¼šé€šè¿‡ providerId æŸ¥è¯¢æ•°æ®åº“ï¼ˆå…¼å®¹ï¼‰
      const currentProviderId = lastContext?.providerId || providerId
      console.warn('ğŸ” [ChatGPT] æŸ¥æ‰¾ä¾›åº”å•†:', currentProviderId)

      try {
        const provider = await getProviderById(currentProviderId!)
        if (provider) {
          providerInfo = {
            baseUrl: provider.base_url,
            apiKey: provider.api_key,
            name: provider.name,
          }
          console.warn('âœ… [ChatGPT] æ‰¾åˆ°ä¾›åº”å•†:', {
            name: providerInfo.name,
            baseUrl: providerInfo.baseUrl,
          })

          // ğŸ”¥ ä½¿ç”¨ä¾›åº”å•†é…ç½®åˆ›å»ºæ–°çš„ API å®ä¾‹
          // ChatGPT API éœ€è¦å®Œæ•´çš„ URLï¼ŒåŒ…æ‹¬ /v1
          const apiBaseUrl = providerInfo.baseUrl.endsWith('/v1')
            ? providerInfo.baseUrl
            : `${providerInfo.baseUrl}/v1`

          const providerOptions: ChatGPTAPIOptions = {
            apiKey: providerInfo.apiKey,
            completionParams: { model: selectedModel },
            debug: !disableDebug,
            messageStore: undefined,
            apiBaseUrl,
            maxModelTokens: 128000,
            maxResponseTokens: maxTokens || 8192,
          }

          setupProxy(providerOptions as any)
          apiInstance = new ChatGPTAPI({ ...providerOptions })
          console.warn('ğŸ”§ [ChatGPT] å·²åˆ›å»ºä¾›åº”å•†ä¸“ç”¨ API å®ä¾‹ï¼ŒURL:', apiBaseUrl)
        }
        else {
          console.warn('âš ï¸ [ChatGPT] æœªæ‰¾åˆ°ä¾›åº”å•†ï¼Œä½¿ç”¨é»˜è®¤é…ç½®')
        }
      }
      catch (error) {
        console.error('âŒ [ChatGPT] æŸ¥æ‰¾ä¾›åº”å•†å¤±è´¥:', error)
        // é™çº§åˆ°é»˜è®¤å®ä¾‹
      }
    }

    // å¦‚æœæ²¡æœ‰ä½¿ç”¨ä¾›åº”å•†é…ç½®ï¼Œåˆ™ä½¿ç”¨åŸæœ‰é€»è¾‘
    if (!providerInfo && isNotEmptyString(selectedModel) && apiModel === 'ChatGPTAPI') {
      apiInstance = createApiForProvider(selectedModel, maxTokens)
    }

    // ğŸ”¥ ç¡®ä¿ apiInstance å·²åˆ›å»º
    if (!apiInstance) {
      throw new Error('API å®ä¾‹æœªåˆ›å»ºï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–æä¾› baseURL å’Œ apiKey')
    }

    // ğŸ”¥ åˆ¤æ–­å½“å‰ä½¿ç”¨çš„ API ç±»å‹
    const currentApiModel = apiInstance instanceof ChatGPTAPI ? 'ChatGPTAPI' : 'ChatGPTUnofficialProxyAPI'

    if (currentApiModel === 'ChatGPTAPI') {
      if (isNotEmptyString(systemMessage))
        options.systemMessage = systemMessage
      // ä½¿ç”¨è¯·æ±‚ä¸­çš„æ¨¡å‹å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
      options.completionParams = {
        model: selectedModel,
        temperature,
        top_p,
        // ğŸ”¥ OpenAI é»˜è®¤å€¼éƒ½æ˜¯ 0ï¼Œæˆ‘ä»¬ä¿æŒé»˜è®¤å³å¯
        // presence_penalty: 0,
        // frequency_penalty: 0,
      }
      // å¦‚æœæä¾›äº† maxTokensï¼Œè®¾ç½® maxResponseTokens
      if (maxTokens && apiInstance instanceof ChatGPTAPI) {
        const chatGptApi = apiInstance as any
        if (chatGptApi.maxResponseTokens !== maxTokens)
          chatGptApi.maxResponseTokens = maxTokens
      }
    }

    if (lastContext != null) {
      if (currentApiModel === 'ChatGPTAPI')
        options.parentMessageId = lastContext.parentMessageId
      else
        options = { ...lastContext }
    }

    console.warn('ğŸ“¤ [ChatGPT] å‡†å¤‡å‘é€è¯·æ±‚:')
    console.warn('   æ¶ˆæ¯:', message.substring(0, 100))
    console.warn('   æ¨¡å‹:', selectedModel)
    console.warn('   é€‰é¡¹:', {
      systemMessage: options.systemMessage?.substring(0, 50),
      completionParams: options.completionParams,
      parentMessageId: options.parentMessageId,
      timeoutMs,
    })

    const startTime = Date.now()

    // ğŸ”¥ æ‰‹åŠ¨ç´¯ç§¯æ–‡æœ¬ï¼ˆä¿®å¤ GLM-4.6 ç­‰æ¨¡å‹çš„ text å­—æ®µä¸ºç©ºé—®é¢˜ï¼‰
    let accumulatedText = ''
    let accumulatedThinkingText = '' // ğŸ”¥ ç´¯ç§¯æ€è€ƒè¿‡ç¨‹

    let progressCallbackCount = 0
    const progressStartTime = Date.now()
    let lastProgressTime = progressStartTime

    const response = await apiInstance.sendMessage(message, {
      ...options,
      onProgress: (partialResponse) => {
        progressCallbackCount++
        const currentTime = Date.now()
        const timeSinceLastProgress = currentTime - lastProgressTime

        if (progressCallbackCount === 1) {
          console.warn(`â±ï¸ [ChatGPT-æ€§èƒ½] é¦–æ¬¡onProgresså›è°ƒ: ${currentTime - progressStartTime}ms`)
        }

        if (timeSinceLastProgress > 100) {
          console.warn(`â±ï¸ [ChatGPT-æ€§èƒ½] ç¬¬${progressCallbackCount}æ¬¡å›è°ƒï¼Œè·ç¦»ä¸Šæ¬¡: ${timeSinceLastProgress}ms`)
        }

        lastProgressTime = currentTime

        // ğŸ”¥ ä» delta æˆ– detail.choices[0].delta.content è·å–å¢é‡å†…å®¹
        const delta = (partialResponse as any).delta || ''
        const content = (partialResponse.detail?.choices?.[0] as any)?.delta?.content || ''
        const reasoningContent = (partialResponse.detail?.choices?.[0] as any)?.delta?.reasoning_content || ''

        // ğŸ”¥ è®°å½•è·³è¿‡çš„æ¬¡æ•°
        let shouldSkip = false
        let skipReason = ''

        // ğŸ”¥ ç´¯ç§¯å®é™…å†…å®¹
        const actualContent = content || delta
        if (actualContent) {
          accumulatedText += actualContent
        }

        // ğŸ”¥ å¤„ç†æ€è€ƒè¿‡ç¨‹ï¼šå¦‚æœæœ‰ reasoning_contentï¼Œä¹Ÿä¼ é€’ç»™å‰ç«¯
        if (reasoningContent && !actualContent) {
          // ğŸ”¥ ç´¯ç§¯æ€è€ƒè¿‡ç¨‹
          accumulatedThinkingText += reasoningContent

          // æ€è€ƒè¿‡ç¨‹ï¼šæ˜¾ç¤ºæ€è€ƒçŠ¶æ€ï¼Œä½†ä¸ç´¯ç§¯åˆ°æœ€ç»ˆæ–‡æœ¬
          const thinkingText = `ğŸ’­ æ€è€ƒä¸­...\n${accumulatedThinkingText}`

          // åˆ›å»ºåŒ…å«æ€è€ƒè¿‡ç¨‹çš„å“åº”å¯¹è±¡
          const thinkingResponse = {
            ...partialResponse,
            text: thinkingText,
            isThinking: true, // æ ‡è®°è¿™æ˜¯æ€è€ƒè¿‡ç¨‹
          }

          processCallback?.(thinkingResponse)
          return
        }

        // å¦‚æœæ—¢æ²¡æœ‰å®é™…å†…å®¹ä¹Ÿæ²¡æœ‰æ€è€ƒå†…å®¹ï¼Œè·³è¿‡
        if (!actualContent && !reasoningContent) {
          shouldSkip = true
          skipReason = 'æ²¡æœ‰å†…å®¹'
        }

        // è®°å½•è·³è¿‡æƒ…å†µ
        if (shouldSkip && progressCallbackCount <= 50) {
          console.warn(`â±ï¸ [ChatGPT-æ€§èƒ½] ç¬¬${progressCallbackCount}æ¬¡è¢«è·³è¿‡ï¼ŒåŸå› : ${skipReason}`)
        }

        if (shouldSkip) {
          return
        }

        // ğŸ”¥ ç¡®ä¿ text å­—æ®µæœ‰å€¼ï¼ˆä¿®å¤å‰ç«¯æ‰“å­—æœºæ•ˆæœï¼‰
        if (!partialResponse.text && accumulatedText) {
          partialResponse.text = accumulatedText
        }

        const callbackStartTime = Date.now()
        processCallback?.(partialResponse)
        const callbackTime = Date.now() - callbackStartTime

        if (callbackTime > 10) {
          console.warn(`â±ï¸ [ChatGPT-æ€§èƒ½] processCallbackè€—æ—¶: ${callbackTime}ms`)
        }

        if (progressCallbackCount <= 20) {
          console.warn(`â±ï¸ [ChatGPT-æ€§èƒ½] ç¬¬${progressCallbackCount}æ¬¡æˆåŠŸè°ƒç”¨processCallbackï¼Œç´¯ç§¯æ–‡æœ¬é•¿åº¦: ${accumulatedText.length}`)
        }
      },
    })
    const endTime = Date.now()

    // eslint-disable-next-line no-console
    console.log('âœ… [ChatGPT] API è°ƒç”¨å®Œæˆ')
    // eslint-disable-next-line no-console
    console.log('â±ï¸ [ChatGPT] è€—æ—¶:', endTime - startTime, 'ms')
    console.warn(`â±ï¸ [ChatGPT-æ€§èƒ½] onProgressæ€»å…±è¢«è°ƒç”¨: ${progressCallbackCount}æ¬¡`)
    // eslint-disable-next-line no-console
    console.log('ğŸ“Š [ChatGPT] å“åº”ä¿¡æ¯:', {
      id: response.id,
      model: response.detail?.model || 'æœªçŸ¥',
      tokens: response.detail?.usage || 'æœªçŸ¥',
    })

    return sendResponse({ type: 'Success', data: response })
  }
  catch (error: any) {
    const code = error.statusCode
    globalThis.console.error(error)
    if (Reflect.has(ErrorCodeMessage, code))
      return sendResponse({ type: 'Fail', message: ErrorCodeMessage[code] })
    return sendResponse({ type: 'Fail', message: error.message ?? 'Please check the back-end console' })
  }
}

async function fetchUsage() {
  const OPENAI_API_KEY = process.env.OPENAI_API_KEY
  const OPENAI_API_BASE_URL = process.env.OPENAI_API_BASE_URL

  if (!isNotEmptyString(OPENAI_API_KEY))
    return Promise.resolve('-')

  const API_BASE_URL = isNotEmptyString(OPENAI_API_BASE_URL)
    ? OPENAI_API_BASE_URL
    : 'https://api.openai.com'

  // è°ƒç”¨ä½™é¢ä¸éœ€è¦åŠ  /v1ï¼Œä½¿ç”¨ /api/usage/token
  const urlUsage = `${API_BASE_URL}/api/usage/token`

  const headers = {
    'Authorization': `Bearer ${OPENAI_API_KEY}`,
    'Content-Type': 'application/json',
  }

  const options = {} as SetProxyOptions

  setupProxy(options)

  try {
    // è·å–å·²ä½¿ç”¨é‡
    const useResponse = await options.fetch(urlUsage, { headers })
    if (!useResponse.ok)
      throw new Error('è·å–ä½¿ç”¨é‡å¤±è´¥')
    const usageData = await useResponse.json() as UsageResponse
    const usage = Math.round(usageData.total_usage) / 100
    return Promise.resolve(usage ? `$${usage}` : '-')
  }
  catch (error) {
    globalThis.console.error(error)
    return Promise.resolve('-')
  }
}

// eslint-disable-next-line unused-imports/no-unused-vars
function formatDate(): string[] {
  const today = new Date()
  const year = today.getFullYear()
  const month = today.getMonth() + 1
  const lastDay = new Date(year, month, 0)
  const formattedFirstDay = `${year}-${month.toString().padStart(2, '0')}-01`
  const formattedLastDay = `${year}-${month.toString().padStart(2, '0')}-${lastDay.getDate().toString().padStart(2, '0')}`
  return [formattedFirstDay, formattedLastDay]
}

async function chatConfig() {
  const usage = await fetchUsage()
  const reverseProxy = process.env.API_REVERSE_PROXY ?? '-'
  const httpsProxy = (process.env.HTTPS_PROXY || process.env.ALL_PROXY) ?? '-'
  const socksProxy = (process.env.SOCKS_PROXY_HOST && process.env.SOCKS_PROXY_PORT)
    ? (`${process.env.SOCKS_PROXY_HOST}:${process.env.SOCKS_PROXY_PORT}`)
    : '-'
  return sendResponse<ModelConfig>({
    type: 'Success',
    data: { apiModel, reverseProxy, timeoutMs, socksProxy, httpsProxy, usage },
  })
}

function setupProxy(options: SetProxyOptions) {
  if (isNotEmptyString(process.env.SOCKS_PROXY_HOST) && isNotEmptyString(process.env.SOCKS_PROXY_PORT)) {
    const agent = new SocksProxyAgent({
      hostname: process.env.SOCKS_PROXY_HOST,
      port: process.env.SOCKS_PROXY_PORT,
      userId: isNotEmptyString(process.env.SOCKS_PROXY_USERNAME) ? process.env.SOCKS_PROXY_USERNAME : undefined,
      password: isNotEmptyString(process.env.SOCKS_PROXY_PASSWORD) ? process.env.SOCKS_PROXY_PASSWORD : undefined,
    })
    options.fetch = (url, options) => {
      return fetch(url, { agent, ...options })
    }
  }
  else if (isNotEmptyString(process.env.HTTPS_PROXY) || isNotEmptyString(process.env.ALL_PROXY)) {
    const httpsProxy = process.env.HTTPS_PROXY || process.env.ALL_PROXY
    if (httpsProxy) {
      const agent = new HttpsProxyAgent(httpsProxy)
      options.fetch = (url, options) => {
        return fetch(url, { agent, ...options })
      }
    }
  }
  else {
    options.fetch = (url, options) => {
      return fetch(url, { ...options })
    }
  }
}

function currentModel(): ApiModel {
  return apiModel || 'ChatGPTAPI'
}

export type { ChatContext, ChatMessage }

export { chatConfig, chatReplyProcess, currentModel }
