import type { ChatGPTAPIOptions, ChatMessage, SendMessageOptions } from 'chatgpt'
import type { ApiModel, ChatContext, ChatGPTUnofficialProxyAPIOptions, ModelConfig } from '../types'
import type { RequestOptions, UsageResponse } from './types'
import type { SetProxyOptions } from './utils'
import { ChatGPTAPI, ChatGPTUnofficialProxyAPI } from 'chatgpt'
import * as dotenv from 'dotenv'
import { getProviderById } from '../db/providerService'
import { sendResponse } from '../utils'
import { isNotEmptyString } from '../utils/is'
import { chatReplyProcessLibrary } from './chatLibrary'
import { chatReplyProcessNative } from './chatNative'
import { setupProxy } from './utils'
import 'isomorphic-fetch'

dotenv.config()

const ErrorCodeMessage: Record<string, string> = {
  401: '[OpenAI] æä¾›é”™è¯¯çš„APIå¯†é’¥ | Incorrect API key provided',
  403: '[OpenAI] æœåŠ¡å™¨æ‹’ç»è®¿é—®ï¼Œè¯·ç¨åå†è¯• | Server refused to access, please try again later',
  502: '[OpenAI] é”™è¯¯çš„ç½‘å…³ |  Bad Gateway',
  503: '[OpenAI] æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åå†è¯• | Server is busy, please try again later',
  504: '[OpenAI] ç½‘å…³è¶…æ—¶ | Gateway Time-out',
  500: '[OpenAI] æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åå†è¯• | Internal Server Error',
}

const timeoutMs: number = !Number.isNaN(+process.env.TIMEOUT_MS) ? +process.env.TIMEOUT_MS : 100 * 1000
const disableDebug: boolean = process.env.OPENAI_API_DISABLE_DEBUG === 'true'

// æŠ‘åˆ¶ chatgpt åº“çš„ token è®¡ç®—é”™è¯¯æ—¥å¿—
const originalConsoleWarn = console.warn
const originalConsoleError = console.error

// è®°å½•æœ€è¿‘çš„é”™è¯¯æ¶ˆæ¯ï¼Œé˜²æ­¢é‡å¤æ‰“å°
const recentErrors = new Set<string>()
const ERROR_CACHE_TIME = 5000 // 5ç§’å†…çš„é‡å¤é”™è¯¯ä¸æ˜¾ç¤º

console.warn = (...args: any[]) => {
  const msg = String(args[0] || '')
  // è¿‡æ»¤æ‰ token è®¡ç®—ç›¸å…³çš„è­¦å‘Š
  if (msg.includes('Failed to calculate number of tokens')
    || msg.includes('falling back to approximate count')) {
    return
  }
  originalConsoleWarn.apply(console, args)
}

console.error = (...args: any[]) => {
  const msg = String(args[0] || '')

  // è¿‡æ»¤æ‰ token è®¡ç®—ç›¸å…³çš„ ECONNRESET é”™è¯¯
  if (msg.includes('Failed to calculate number of tokens'))
    return

  // é˜²æ­¢çŸ­æ—¶é—´å†…é‡å¤æ‰“å°ç›¸åŒçš„é”™è¯¯
  const errorKey = msg.substring(0, 100)
  if (recentErrors.has(errorKey))
    return

  recentErrors.add(errorKey)
  setTimeout(() => recentErrors.delete(errorKey), ERROR_CACHE_TIME)

  originalConsoleError.apply(console, args)
}

let apiModel: ApiModel
let api: ChatGPTAPI | ChatGPTUnofficialProxyAPI
let isInitialized = false

// å»¶è¿Ÿåˆå§‹åŒ–å‡½æ•°ï¼ˆå¯é€‰ï¼šä»…åœ¨ä½¿ç”¨ç¯å¢ƒå˜é‡æ—¶éœ€è¦ï¼‰
async function initializeAPI() {
  if (isInitialized)
    return

  const model = isNotEmptyString(process.env.OPENAI_API_MODEL) ? process.env.OPENAI_API_MODEL : 'gpt-3.5-turbo'

  // æ–°æ¶æ„ï¼šä¼˜å…ˆä½¿ç”¨æ•°æ®åº“é…ç½®ï¼Œç¯å¢ƒå˜é‡ä½œä¸ºåå¤‡
  if (!isNotEmptyString(process.env.OPENAI_API_KEY) && !isNotEmptyString(process.env.OPENAI_ACCESS_TOKEN)) {
    isInitialized = true
    return
  }

  // å¦‚æœé…ç½®äº†ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡åˆå§‹åŒ–ï¼ˆå‘åå…¼å®¹ï¼‰
  console.warn('âœ… [ChatGPT] æ£€æµ‹åˆ°ç¯å¢ƒå˜é‡é…ç½®ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡åˆå§‹åŒ– API')

  await (async () => {
    if (isNotEmptyString(process.env.OPENAI_API_KEY)) {
      const OPENAI_API_BASE_URL = process.env.OPENAI_API_BASE_URL

      const options: ChatGPTAPIOptions = {
        apiKey: process.env.OPENAI_API_KEY,
        completionParams: { model },
        debug: !disableDebug,
      }

      // increase max token limit if use gpt-4
      if (model.toLowerCase().includes('gpt-4')) {
        if (model.toLowerCase().includes('32k')) {
          options.maxModelTokens = 32768
          options.maxResponseTokens = 8192
        }
        else if (/-4o-mini/.test(model.toLowerCase())) {
          options.maxModelTokens = 128000
          options.maxResponseTokens = 16384
        }
        else if (/-preview|-turbo|o/.test(model.toLowerCase())) {
          options.maxModelTokens = 128000
          options.maxResponseTokens = 4096
        }
        else {
          options.maxModelTokens = 8192
          options.maxResponseTokens = 2048
        }
      }
      else if (model.toLowerCase().includes('gpt-3.5')) {
        if (/16k|1106|0125/.test(model.toLowerCase())) {
          options.maxModelTokens = 16384
          options.maxResponseTokens = 4096
        }
      }

      if (isNotEmptyString(OPENAI_API_BASE_URL)) {
        options.apiBaseUrl = `${OPENAI_API_BASE_URL}/v1`
      }

      setupProxy(options as any)

      api = new ChatGPTAPI({ ...options })
      apiModel = 'ChatGPTAPI'
    }
    else {
      const options: ChatGPTUnofficialProxyAPIOptions = {
        accessToken: process.env.OPENAI_ACCESS_TOKEN,
        apiReverseProxyUrl: isNotEmptyString(process.env.API_REVERSE_PROXY) ? process.env.API_REVERSE_PROXY : 'https://ai.fakeopen.com/api/conversation',
        model,
        debug: !disableDebug,
      }

      setupProxy(options as any)

      api = new ChatGPTUnofficialProxyAPI({ ...options })
      apiModel = 'ChatGPTUnofficialProxyAPI'
    }
  })()

  isInitialized = true
}

// åˆ¤æ–­æ¨¡å‹æ˜¯å¦ä¸º Kriora ä¾›åº”å•†
function isKrioraModel(modelId: string): boolean {
  return modelId.includes('moonshotai/') || modelId.includes('qwen/')
}

// ä¸ºç‰¹å®šä¾›åº”å•†åˆ›å»º API å®ä¾‹
function createApiForProvider(modelId: string, maxTokens?: number): ChatGPTAPI {
  if (isKrioraModel(modelId)) {
    const krioraApiKey = process.env.KRIORA_API_KEY || process.env.OPENAI_API_KEY
    const krioraApiUrl = process.env.KRIORA_API_URL || 'https://api.kriora.com'

    const options: ChatGPTAPIOptions = {
      apiKey: krioraApiKey,
      completionParams: { model: modelId },
      debug: !disableDebug,
      apiBaseUrl: `${krioraApiUrl}/v1`,
      maxModelTokens: 128000,
      maxResponseTokens: maxTokens || 8192,
    }

    setupProxy(options as any)
    return new ChatGPTAPI({ ...options })
  }

  return api as ChatGPTAPI
}

/**
 * èŠå¤©å›å¤å¤„ç†ï¼ˆä¸»å…¥å£ï¼‰
 */
async function chatReplyProcess(options: RequestOptions) {
  // ç¡®ä¿APIå·²åˆå§‹åŒ–
  await initializeAPI()

  const { message, lastContext, historyMessages, process: processCallback, systemMessage, temperature, top_p, model: requestModel, maxTokens, providerId, baseURL, apiKey } = options

  try {
    const defaultModel = isNotEmptyString(process.env.OPENAI_API_MODEL) ? process.env.OPENAI_API_MODEL : 'gpt-3.5-turbo'
    const selectedModel = requestModel || defaultModel

    // ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ç›´æ¥ä¼ é€’çš„ baseURL å’Œ apiKeyï¼ˆæ–°æ–¹å¼ï¼‰
    let apiInstance: ChatGPTAPI | ChatGPTUnofficialProxyAPI | null = api
    let providerInfo: { baseUrl: string, apiKey: string, name: string } | null = null

    if (baseURL && apiKey) {
      // æ–°æ–¹å¼ï¼šç›´æ¥ä½¿ç”¨ä¼ é€’çš„é…ç½®
      providerInfo = {
        baseUrl: baseURL,
        apiKey,
        name: 'Direct Config',
      }
    }
    else if (lastContext?.providerId || providerId) {
      // æ—§æ–¹å¼ï¼šé€šè¿‡ providerId æŸ¥è¯¢æ•°æ®åº“ï¼ˆå…¼å®¹ï¼‰
      const currentProviderId = lastContext?.providerId || providerId
      console.warn('ğŸ” [ChatGPT] æŸ¥æ‰¾ä¾›åº”å•†:', currentProviderId)

      try {
        const provider = await getProviderById(currentProviderId!)
        if (provider) {
          providerInfo = {
            baseUrl: provider.base_url,
            apiKey: provider.api_key,
            name: provider.name,
          }
          console.warn('âœ… [ChatGPT] æ‰¾åˆ°ä¾›åº”å•†:', {
            name: providerInfo.name,
            baseUrl: providerInfo.baseUrl,
          })
        }
        else {
          console.warn('âš ï¸ [ChatGPT] æœªæ‰¾åˆ°ä¾›åº”å•†ï¼Œä½¿ç”¨é»˜è®¤é…ç½®')
        }
      }
      catch (error) {
        console.error('âŒ [ChatGPT] æŸ¥æ‰¾ä¾›åº”å•†å¤±è´¥:', error)
      }
    }

    // ğŸ”¥ ä½¿ç”¨æ–°æ¶æ„çš„å®ç°ï¼ˆç»Ÿä¸€å‚æ•°ï¼Œå†…éƒ¨æ ¹æ®æƒ…å†µé€‰æ‹©è°ƒç”¨æ–¹å¼ï¼‰
    if (providerInfo) {
      // å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é€‰æ‹©ä½¿ç”¨å“ªä¸ªå®ç°
      const useNativeImplementation = process.env.USE_NATIVE_CHAT === 'true'

      if (useNativeImplementation) {
        // ä½¿ç”¨åŸç”Ÿ fetch å®ç°
        return await chatReplyProcessNative({
          message,
          historyMessages: historyMessages || [],
          baseURL: providerInfo.baseUrl,
          apiKey: providerInfo.apiKey,
          model: selectedModel,
          temperature,
          top_p,
          maxTokens,
          processCallback,
        })
      }
      else {
        // ä½¿ç”¨ ChatGPT åº“å®ç°ï¼ˆé»˜è®¤ï¼‰
        return await chatReplyProcessLibrary({
          message,
          historyMessages,
          lastContext,
          systemMessage,
          temperature,
          top_p,
          model: selectedModel,
          maxTokens,
          baseURL: providerInfo.baseUrl,
          apiKey: providerInfo.apiKey,
          processCallback,
        })
      }
    }

    // å¦‚æœæ²¡æœ‰ä½¿ç”¨ä¾›åº”å•†é…ç½®ï¼Œåˆ™ä½¿ç”¨åŸæœ‰é€»è¾‘
    if (isNotEmptyString(selectedModel) && apiModel === 'ChatGPTAPI') {
      apiInstance = createApiForProvider(selectedModel, maxTokens)
    }

    if (!apiInstance) {
      throw new Error('API å®ä¾‹æœªåˆ›å»ºï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–æä¾› baseURL å’Œ apiKey')
    }

    // ä½¿ç”¨é»˜è®¤ API å®ä¾‹ï¼ˆç¯å¢ƒå˜é‡é…ç½®ï¼‰
    const currentApiModel = apiInstance instanceof ChatGPTAPI ? 'ChatGPTAPI' : 'ChatGPTUnofficialProxyAPI'

    let sendOptions: SendMessageOptions = { timeoutMs }

    if (currentApiModel === 'ChatGPTAPI') {
      if (isNotEmptyString(systemMessage))
        sendOptions.systemMessage = systemMessage

      sendOptions.completionParams = {
        model: selectedModel,
        temperature,
        top_p,
      }

      if (maxTokens && apiInstance instanceof ChatGPTAPI) {
        const chatGptApi = apiInstance as any
        if (chatGptApi.maxResponseTokens !== maxTokens)
          chatGptApi.maxResponseTokens = maxTokens
      }
    }

    if (lastContext != null) {
      if (currentApiModel === 'ChatGPTAPI')
        sendOptions.parentMessageId = lastContext.parentMessageId
      else
        sendOptions = { ...lastContext }
    }

    const startTime = Date.now()
    const response = await apiInstance.sendMessage(message, {
      ...sendOptions,
      onProgress: (partialResponse) => {
        processCallback?.(partialResponse)
      },
    })

    const responseTime = Date.now() - startTime
    console.warn('ğŸ“Š [ChatGPT] å“åº”ä¿¡æ¯:', {
      time: `${responseTime}ms`,
      id: response.id,
      model: response.detail?.model || 'æœªçŸ¥',
      tokens: response.detail?.usage || 'æœªçŸ¥',
    })

    return sendResponse({ type: 'Success', data: response })
  }
  catch (error: any) {
    const code = error.statusCode
    globalThis.console.error(error)
    if (Reflect.has(ErrorCodeMessage, code))
      return sendResponse({ type: 'Fail', message: ErrorCodeMessage[code] })
    return sendResponse({ type: 'Fail', message: error.message ?? 'Please check the back-end console' })
  }
}

async function fetchUsage() {
  const OPENAI_API_KEY = process.env.OPENAI_API_KEY
  const OPENAI_API_BASE_URL = process.env.OPENAI_API_BASE_URL

  if (!isNotEmptyString(OPENAI_API_KEY))
    return Promise.resolve('-')

  const API_BASE_URL = isNotEmptyString(OPENAI_API_BASE_URL)
    ? OPENAI_API_BASE_URL
    : 'https://api.openai.com'

  const urlUsage = `${API_BASE_URL}/api/usage/token`

  const headers = {
    'Authorization': `Bearer ${OPENAI_API_KEY}`,
    'Content-Type': 'application/json',
  }

  const options = {} as SetProxyOptions

  setupProxy(options)

  try {
    const useResponse = await options.fetch(urlUsage, { headers })
    if (!useResponse.ok)
      throw new Error('è·å–ä½¿ç”¨é‡å¤±è´¥')
    const usageData = await useResponse.json() as UsageResponse
    const usage = Math.round(usageData.total_usage) / 100
    return Promise.resolve(usage ? `$${usage}` : '-')
  }
  catch (error) {
    globalThis.console.error(error)
    return Promise.resolve('-')
  }
}

async function chatConfig() {
  const usage = await fetchUsage()
  const reverseProxy = process.env.API_REVERSE_PROXY ?? '-'
  const httpsProxy = (process.env.HTTPS_PROXY || process.env.ALL_PROXY) ?? '-'
  const socksProxy = (process.env.SOCKS_PROXY_HOST && process.env.SOCKS_PROXY_PORT)
    ? (`${process.env.SOCKS_PROXY_HOST}:${process.env.SOCKS_PROXY_PORT}`)
    : '-'
  return sendResponse<ModelConfig>({
    type: 'Success',
    data: { apiModel, reverseProxy, timeoutMs, socksProxy, httpsProxy, usage },
  })
}

function currentModel(): ApiModel {
  return apiModel || 'ChatGPTAPI'
}

export type { ChatContext, ChatMessage }
export { chatConfig, chatReplyProcess, currentModel }
