import type { ChatGPTAPIOptions, ChatMessage, SendMessageOptions } from 'chatgpt'
import type { ApiModel, ChatContext, ChatGPTUnofficialProxyAPIOptions, ModelConfig } from '../types'
import type { RequestOptions, SetProxyOptions, UsageResponse } from './types'
import { ChatGPTAPI, ChatGPTUnofficialProxyAPI } from 'chatgpt'
import * as dotenv from 'dotenv'
import httpsProxyAgent from 'https-proxy-agent'
import fetch from 'node-fetch'
import { SocksProxyAgent } from 'socks-proxy-agent'
import { sendResponse } from '../utils'
import { isNotEmptyString } from '../utils/is'
import 'isomorphic-fetch'

const { HttpsProxyAgent } = httpsProxyAgent

dotenv.config()

const ErrorCodeMessage: Record<string, string> = {
  401: '[OpenAI] æä¾›é”™è¯¯çš„APIå¯†é’¥ | Incorrect API key provided',
  403: '[OpenAI] æœåŠ¡å™¨æ‹’ç»è®¿é—®ï¼Œè¯·ç¨åå†è¯• | Server refused to access, please try again later',
  502: '[OpenAI] é”™è¯¯çš„ç½‘å…³ |  Bad Gateway',
  503: '[OpenAI] æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åå†è¯• | Server is busy, please try again later',
  504: '[OpenAI] ç½‘å…³è¶…æ—¶ | Gateway Time-out',
  500: '[OpenAI] æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åå†è¯• | Internal Server Error',
}

const timeoutMs: number = !Number.isNaN(+process.env.TIMEOUT_MS) ? +process.env.TIMEOUT_MS : 100 * 1000
const disableDebug: boolean = process.env.OPENAI_API_DISABLE_DEBUG === 'true'

// æŠ‘åˆ¶ chatgpt åº“çš„ token è®¡ç®—é”™è¯¯æ—¥å¿—
// è¿™äº›é”™è¯¯é€šå¸¸æ˜¯ç”±äºç½‘ç»œé—®é¢˜å¯¼è‡´çš„ tiktoken æ¨¡å‹ä¸‹è½½å¤±è´¥
const originalConsoleWarn = console.warn
const originalConsoleError = console.error

// è®°å½•æœ€è¿‘çš„é”™è¯¯æ¶ˆæ¯ï¼Œé˜²æ­¢é‡å¤æ‰“å°
const recentErrors = new Set<string>()
const ERROR_CACHE_TIME = 5000 // 5ç§’å†…çš„é‡å¤é”™è¯¯ä¸æ˜¾ç¤º

console.warn = (...args: any[]) => {
  const msg = String(args[0] || '')
  // è¿‡æ»¤æ‰ token è®¡ç®—ç›¸å…³çš„è­¦å‘Š
  if (msg.includes('Failed to calculate number of tokens')
    || msg.includes('falling back to approximate count')) {
    return
  }

  originalConsoleWarn.apply(console, args)
}

console.error = (...args: any[]) => {
  const msg = String(args[0] || '')

  // è¿‡æ»¤æ‰ token è®¡ç®—ç›¸å…³çš„ ECONNRESET é”™è¯¯
  if (msg.includes('Failed to calculate number of tokens'))
    return

  // é˜²æ­¢çŸ­æ—¶é—´å†…é‡å¤æ‰“å°ç›¸åŒçš„é”™è¯¯
  const errorKey = msg.substring(0, 100)
  if (recentErrors.has(errorKey))
    return

  recentErrors.add(errorKey)
  setTimeout(() => recentErrors.delete(errorKey), ERROR_CACHE_TIME)

  originalConsoleError.apply(console, args)
}

let apiModel: ApiModel
const model = isNotEmptyString(process.env.OPENAI_API_MODEL) ? process.env.OPENAI_API_MODEL : 'gpt-3.5-turbo'

if (!isNotEmptyString(process.env.OPENAI_API_KEY) && !isNotEmptyString(process.env.OPENAI_ACCESS_TOKEN))
  throw new Error('Missing OPENAI_API_KEY or OPENAI_ACCESS_TOKEN environment variable')

let api: ChatGPTAPI | ChatGPTUnofficialProxyAPI

(async () => {
  // More Info: https://github.com/transitive-bullshit/chatgpt-api

  if (isNotEmptyString(process.env.OPENAI_API_KEY)) {
    const OPENAI_API_BASE_URL = process.env.OPENAI_API_BASE_URL

    const options: ChatGPTAPIOptions = {
      apiKey: process.env.OPENAI_API_KEY,
      completionParams: { model },
      debug: !disableDebug,
      // ç¦ç”¨ token è®¡æ•°ä»¥é¿å…ç½‘ç»œé”™è¯¯
      // chatgptåº“ä¼šå°è¯•ä»ç½‘ç»œä¸‹è½½tiktokenæ¨¡å‹ï¼Œå¯èƒ½å¯¼è‡´ECONNRESETé”™è¯¯
      messageStore: undefined,
    }

    // increase max token limit if use gpt-4
    if (model.toLowerCase().includes('gpt-4')) {
      // if use 32k model
      if (model.toLowerCase().includes('32k')) {
        options.maxModelTokens = 32768
        options.maxResponseTokens = 8192
      }
      else if (/-4o-mini/.test(model.toLowerCase())) {
        options.maxModelTokens = 128000
        options.maxResponseTokens = 16384
      }
      // if use GPT-4 Turbo or GPT-4o
      else if (/-preview|-turbo|o/.test(model.toLowerCase())) {
        options.maxModelTokens = 128000
        options.maxResponseTokens = 4096
      }
      else {
        options.maxModelTokens = 8192
        options.maxResponseTokens = 2048
      }
    }
    else if (model.toLowerCase().includes('gpt-3.5')) {
      if (/16k|1106|0125/.test(model.toLowerCase())) {
        options.maxModelTokens = 16384
        options.maxResponseTokens = 4096
      }
    }

    if (isNotEmptyString(OPENAI_API_BASE_URL)) {
      // æ¨¡å‹è°ƒç”¨éœ€è¦åŠ  /v1
      options.apiBaseUrl = `${OPENAI_API_BASE_URL}/v1`
    }

    setupProxy(options)

    api = new ChatGPTAPI({ ...options })
    apiModel = 'ChatGPTAPI'
  }
  else {
    const options: ChatGPTUnofficialProxyAPIOptions = {
      accessToken: process.env.OPENAI_ACCESS_TOKEN,
      apiReverseProxyUrl: isNotEmptyString(process.env.API_REVERSE_PROXY) ? process.env.API_REVERSE_PROXY : 'https://ai.fakeopen.com/api/conversation',
      model,
      debug: !disableDebug,
    }

    setupProxy(options)

    api = new ChatGPTUnofficialProxyAPI({ ...options })
    apiModel = 'ChatGPTUnofficialProxyAPI'
  }
})()

async function chatReplyProcess(options: RequestOptions) {
  const { message, lastContext, process, systemMessage, temperature, top_p, model: requestModel } = options
  try {
    let options: SendMessageOptions = { timeoutMs }

    if (apiModel === 'ChatGPTAPI') {
      if (isNotEmptyString(systemMessage))
        options.systemMessage = systemMessage
      // ä½¿ç”¨è¯·æ±‚ä¸­çš„æ¨¡å‹å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
      const selectedModel = requestModel || model
      options.completionParams = { model: selectedModel, temperature, top_p }
    }

    if (lastContext != null) {
      if (apiModel === 'ChatGPTAPI')
        options.parentMessageId = lastContext.parentMessageId
      else
        options = { ...lastContext }
    }

    // æ·»åŠ è°ƒè¯•ä¿¡æ¯
    // eslint-disable-next-line no-console
    console.log('ğŸš€ [ChatGPT] å¼€å§‹è°ƒç”¨ API')
    // eslint-disable-next-line no-console
    console.log('ğŸ“ [ChatGPT] æ¶ˆæ¯å†…å®¹:', message)
    // eslint-disable-next-line no-console
    console.log('âš™ï¸ [ChatGPT] è¯·æ±‚é€‰é¡¹:', {
      model: options.completionParams?.model || 'æœªæŒ‡å®š',
      systemMessage: options.systemMessage || 'æ— ',
      temperature: options.completionParams?.temperature,
      top_p: options.completionParams?.top_p,
      parentMessageId: options.parentMessageId || 'æ— ä¸Šä¸‹æ–‡',
    })

    const startTime = Date.now()
    const response = await api.sendMessage(message, {
      ...options,
      onProgress: (partialResponse) => {
        process?.(partialResponse)
      },
    })
    const endTime = Date.now()

    // eslint-disable-next-line no-console
    console.log('âœ… [ChatGPT] API è°ƒç”¨å®Œæˆ')
    // eslint-disable-next-line no-console
    console.log('â±ï¸ [ChatGPT] è€—æ—¶:', endTime - startTime, 'ms')
    // eslint-disable-next-line no-console
    console.log('ğŸ“Š [ChatGPT] å“åº”ä¿¡æ¯:', {
      id: response.id,
      model: response.detail?.model || 'æœªçŸ¥',
      tokens: response.detail?.usage || 'æœªçŸ¥',
    })

    return sendResponse({ type: 'Success', data: response })
  }
  catch (error: any) {
    const code = error.statusCode
    globalThis.console.error(error)
    if (Reflect.has(ErrorCodeMessage, code))
      return sendResponse({ type: 'Fail', message: ErrorCodeMessage[code] })
    return sendResponse({ type: 'Fail', message: error.message ?? 'Please check the back-end console' })
  }
}

async function fetchUsage() {
  const OPENAI_API_KEY = process.env.OPENAI_API_KEY
  const OPENAI_API_BASE_URL = process.env.OPENAI_API_BASE_URL

  if (!isNotEmptyString(OPENAI_API_KEY))
    return Promise.resolve('-')

  const API_BASE_URL = isNotEmptyString(OPENAI_API_BASE_URL)
    ? OPENAI_API_BASE_URL
    : 'https://api.openai.com'

  // è°ƒç”¨ä½™é¢ä¸éœ€è¦åŠ  /v1ï¼Œä½¿ç”¨ /api/usage/token
  const urlUsage = `${API_BASE_URL}/api/usage/token`

  const headers = {
    'Authorization': `Bearer ${OPENAI_API_KEY}`,
    'Content-Type': 'application/json',
  }

  const options = {} as SetProxyOptions

  setupProxy(options)

  try {
    // è·å–å·²ä½¿ç”¨é‡
    const useResponse = await options.fetch(urlUsage, { headers })
    if (!useResponse.ok)
      throw new Error('è·å–ä½¿ç”¨é‡å¤±è´¥')
    const usageData = await useResponse.json() as UsageResponse
    const usage = Math.round(usageData.total_usage) / 100
    return Promise.resolve(usage ? `$${usage}` : '-')
  }
  catch (error) {
    globalThis.console.error(error)
    return Promise.resolve('-')
  }
}

// eslint-disable-next-line unused-imports/no-unused-vars
function formatDate(): string[] {
  const today = new Date()
  const year = today.getFullYear()
  const month = today.getMonth() + 1
  const lastDay = new Date(year, month, 0)
  const formattedFirstDay = `${year}-${month.toString().padStart(2, '0')}-01`
  const formattedLastDay = `${year}-${month.toString().padStart(2, '0')}-${lastDay.getDate().toString().padStart(2, '0')}`
  return [formattedFirstDay, formattedLastDay]
}

async function chatConfig() {
  const usage = await fetchUsage()
  const reverseProxy = process.env.API_REVERSE_PROXY ?? '-'
  const httpsProxy = (process.env.HTTPS_PROXY || process.env.ALL_PROXY) ?? '-'
  const socksProxy = (process.env.SOCKS_PROXY_HOST && process.env.SOCKS_PROXY_PORT)
    ? (`${process.env.SOCKS_PROXY_HOST}:${process.env.SOCKS_PROXY_PORT}`)
    : '-'
  return sendResponse<ModelConfig>({
    type: 'Success',
    data: { apiModel, reverseProxy, timeoutMs, socksProxy, httpsProxy, usage },
  })
}

function setupProxy(options: SetProxyOptions) {
  if (isNotEmptyString(process.env.SOCKS_PROXY_HOST) && isNotEmptyString(process.env.SOCKS_PROXY_PORT)) {
    const agent = new SocksProxyAgent({
      hostname: process.env.SOCKS_PROXY_HOST,
      port: process.env.SOCKS_PROXY_PORT,
      userId: isNotEmptyString(process.env.SOCKS_PROXY_USERNAME) ? process.env.SOCKS_PROXY_USERNAME : undefined,
      password: isNotEmptyString(process.env.SOCKS_PROXY_PASSWORD) ? process.env.SOCKS_PROXY_PASSWORD : undefined,
    })
    options.fetch = (url, options) => {
      return fetch(url, { agent, ...options })
    }
  }
  else if (isNotEmptyString(process.env.HTTPS_PROXY) || isNotEmptyString(process.env.ALL_PROXY)) {
    const httpsProxy = process.env.HTTPS_PROXY || process.env.ALL_PROXY
    if (httpsProxy) {
      const agent = new HttpsProxyAgent(httpsProxy)
      options.fetch = (url, options) => {
        return fetch(url, { agent, ...options })
      }
    }
  }
  else {
    options.fetch = (url, options) => {
      return fetch(url, { ...options })
    }
  }
}

function currentModel(): ApiModel {
  return apiModel
}

export type { ChatContext, ChatMessage }

export { chatConfig, chatReplyProcess, currentModel }
