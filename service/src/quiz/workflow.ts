/* eslint-disable no-console */
import type { ClassificationLabel, HumanFeedbackInput, ModelConfig, ModelInfo, QuizItem, Subject, WorkflowNodeConfig, WorkflowNodeType, WorkflowState } from './types'
// workflow.ts
import { writeFile } from 'node:fs/promises'
import { join } from 'node:path'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { END, StateGraph } from '@langchain/langgraph'
import { ChatOpenAI } from '@langchain/openai'
import { loadFile } from './loader'

// ---------- LLM ----------
// åˆ¤æ–­æ¨¡å‹æ˜¯å¦ä¸º Kriora ä¾›åº”å•†
function isKrioraModel(modelId: string): boolean {
  return modelId.includes('moonshotai/') || modelId.includes('qwen/')
}

function makeLLM(modelInfo?: ModelInfo, config?: ModelConfig) {
  // å¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹ä¿¡æ¯ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
  const model = modelInfo?.name || process.env.OPENAI_API_MODEL || 'gpt-4o-mini'

  // æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„ API é…ç½®
  let apiKey = modelInfo?.apiKey
  let baseURL = modelInfo?.baseURL

  if (!apiKey || !baseURL) {
    if (isKrioraModel(model)) {
      // ä½¿ç”¨ Kriora API é…ç½®
      apiKey = apiKey || process.env.KRIORA_API_KEY || process.env.OPENAI_API_KEY
      baseURL = baseURL || process.env.KRIORA_API_URL || 'https://api.kriora.com'
    }
    else {
      // ä½¿ç”¨é»˜è®¤ OpenAI API é…ç½®
      apiKey = apiKey || process.env.OPENAI_API_KEY
      baseURL = baseURL || process.env.OPENAI_API_BASE_URL
    }
  }

  console.log('ğŸ”‘ [LLMé…ç½®]', {
    model,
    baseURL,
    hasApiKey: !!apiKey,
    provider: isKrioraModel(model) ? 'kriora' : (modelInfo?.provider || 'openai'),
    config,
  })

  if (!apiKey)
    throw new Error('API_KEY æœªé…ç½®ï¼è¯·åœ¨ service/.env æ–‡ä»¶ä¸­é…ç½®æˆ–é€šè¿‡å·¥ä½œæµé…ç½®ä¼ å…¥')

  return new ChatOpenAI({
    model,
    temperature: config?.temperature ?? 0,
    topP: config?.top_p,
    maxTokens: config?.max_tokens,
    presencePenalty: config?.presence_penalty,
    frequencyPenalty: config?.frequency_penalty,
    openAIApiKey: apiKey,
    configuration: {
      // æ¨¡å‹è°ƒç”¨éœ€è¦åŠ  /v1
      baseURL: baseURL ? `${baseURL}/v1` : 'https://api.openai.com/v1',
    },
    streaming: true,
    timeout: 60000,
  })
}

// è·å–èŠ‚ç‚¹é…ç½®
function getNodeConfig(state: WorkflowState, nodeType: WorkflowNodeType): { modelInfo: ModelInfo, config: ModelConfig } | null {
  const nodeConfig = state.workflowConfig?.find(c => c.nodeType === nodeType)
  if (!nodeConfig)
    return null

  // å¦‚æœæœ‰å­¦ç§‘ä¿¡æ¯ä¸”è¯¥èŠ‚ç‚¹æœ‰å­¦ç§‘ä¸“å±é…ç½®ï¼Œä½¿ç”¨å­¦ç§‘ä¸“å±æ¨¡å‹
  if (state.subject && state.subject !== 'unknown' && nodeConfig.subjectSpecific?.[state.subject]) {
    return {
      modelInfo: nodeConfig.subjectSpecific[state.subject]!,
      config: nodeConfig.config || {},
    }
  }

  return {
    modelInfo: nodeConfig.modelInfo,
    config: nodeConfig.config || {},
  }
}

// ---------- 1. åˆ†ç±»å™¨ï¼ˆå¢å¼ºï¼šåŒæ—¶è¯†åˆ«å­¦ç§‘ï¼‰ ----------
const classifierPrompt = ChatPromptTemplate.fromMessages([
  [
    'system',
    `ä½ æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œåˆ¤æ–­è¾“å…¥æ–‡æœ¬çš„ç±»å‹å’Œå­¦ç§‘é—¨ç±»ã€‚

ç¬¬ä¸€æ­¥ï¼šåˆ¤æ–­æ–‡æœ¬ç±»å‹
- å¦‚æœä¸»è¦æ˜¯çŸ¥è¯†ç‚¹ã€æ¦‚å¿µã€è¯´æ˜ç­‰å†…å®¹ï¼Œç±»å‹ä¸º 'note'
- å¦‚æœä¸»è¦æ˜¯é¢˜ç›®ï¼ˆåŒ…å«é¢˜å¹²ã€é€‰é¡¹ã€ç­”æ¡ˆï¼‰ï¼Œç±»å‹ä¸º 'question'
- å¦‚æœåŒæ—¶åŒ…å«å¤§é‡ç¬”è®°å’Œé¢˜ç›®ï¼Œç±»å‹ä¸º 'mixed'

ç¬¬äºŒæ­¥ï¼šåˆ¤æ–­å­¦ç§‘é—¨ç±»ï¼ˆå¦‚æœèƒ½åˆ¤æ–­ï¼‰
- math: æ•°å­¦
- physics: ç‰©ç†
- chemistry: åŒ–å­¦
- biology: ç”Ÿç‰©
- chinese: è¯­æ–‡
- english: è‹±è¯­
- unknown: æ— æ³•åˆ¤æ–­æˆ–å¤šå­¦ç§‘æ··åˆ

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆåªè¾“å‡ºè¿™ä¸¤è¡Œï¼‰ï¼š
type: <note|question|mixed>
subject: <math|physics|chemistry|biology|chinese|english|unknown>`,
  ],
  ['human', '{text}'],
])

async function classify(state: WorkflowState): Promise<WorkflowState> {
  console.log('ğŸ¤– [åˆ†ç±»å™¨] å¼€å§‹è°ƒç”¨ LLM è¿›è¡Œåˆ†ç±»...')
  console.log('ğŸ“ [åˆ†ç±»å™¨] æ–‡æœ¬é¢„è§ˆ (å‰100å­—):', state.text.slice(0, 100))

  try {
    const nodeConfig = getNodeConfig(state, 'classify')
    const llm = nodeConfig
      ? makeLLM(nodeConfig.modelInfo, nodeConfig.config)
      : makeLLM()

    const chain = classifierPrompt.pipe(llm)
    const textSample = state.text.slice(0, 3000)

    console.log('ğŸ”„ [åˆ†ç±»å™¨] å‘é€æ–‡æœ¬ç»™ LLMï¼Œé•¿åº¦:', textSample.length)

    const result = await chain.invoke({ text: textSample })
    const response = (result.content as string).trim().toLowerCase()

    console.log('ğŸ“Š [åˆ†ç±»å™¨] LLM è¿”å›ç»“æœ:', response)

    // è§£æå“åº”
    const typeMatch = response.match(/type:\s*(note|question|mixed|unknown)/)
    const subjectMatch = response.match(/subject:\s*(math|physics|chemistry|biology|chinese|english|unknown)/)

    const type = typeMatch?.[1] || 'unknown'
    const subject = subjectMatch?.[1] || 'unknown'

    state.classification = type as ClassificationLabel
    state.subject = subject as Subject

    console.log('âœ… [åˆ†ç±»å™¨] åˆ†ç±»ç»“æœ:', {
      type: state.classification,
      subject: state.subject,
    })

    if (state.classification === 'mixed')
      state.error = 'æ–‡ä»¶åŒæ—¶åŒ…å«ç¬”è®°å’Œé¢˜ç›®ï¼Œè¯·åˆ†å¼€å¤„ç†'
  }
  catch (error: any) {
    console.error('âŒ [åˆ†ç±»å™¨] API è°ƒç”¨å¤±è´¥:', error)
    console.error('é”™è¯¯è¯¦æƒ…:', {
      name: error?.name,
      message: error?.message,
      code: error?.code,
      status: error?.status,
      response: error?.response?.data,
    })

    state.classification = 'note'
    state.subject = 'unknown'
    state.error = `API è°ƒç”¨å¤±è´¥: ${error?.message || String(error)}`

    throw error
  }

  return state
}

async function parseQuestions(state: WorkflowState): Promise<WorkflowState> {
  const nodeConfig = getNodeConfig(state, 'parse_questions')
  const llm = nodeConfig
    ? makeLLM(nodeConfig.modelInfo, nodeConfig.config)
    : makeLLM()

  // åŠ¨æ€æ‹¼æ¥ç³»ç»Ÿæç¤º
  const systemPrompt = `ä½ æ˜¯é¢˜ç›®è§£æåŠ©æ‰‹ã€‚å°†åŸå§‹é¢˜ç›®æ–‡æœ¬è§£æä¸ºæ ‡å‡†JSONæ ¼å¼ã€‚
    æ¯é“é¢˜å¿…é¡»åŒ…å«ï¼š
    - type: "single_choice" | "multiple_choice" | "true_false"
    - question: é¢˜ç›®å†…å®¹
    - options: é€‰é¡¹æ•°ç»„ ["A. ...", "B. ..."]
    - answer: æ­£ç¡®ç­”æ¡ˆï¼ˆå¦‚ ["A"] æˆ– ["A","B"]ï¼‰
    - explanation: ç­”æ¡ˆè§£æ
    ${state.revision_note ? `\nç”¨æˆ·ä¿®æ”¹å»ºè®®ï¼š${state.revision_note}\nè¯·æ ¹æ®å»ºè®®è°ƒæ•´è¾“å‡ºã€‚` : ''}
    
    ç›´æ¥è¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•é¢å¤–è¯´æ˜ã€‚`
  const prompt = ChatPromptTemplate.fromMessages([
    ['system', systemPrompt],
    ['human', '{text}'],
  ])

  const chain = prompt.pipe(llm)

  const promptText = state.revision_note
    ? `åŸå§‹é¢˜ç›®ï¼š\n${state.text}\n\nä¹‹å‰çš„è§£æç»“æœï¼š\n${JSON.stringify(state.questions, null, 2)}\n\nä¿®æ”¹å»ºè®®ï¼š${state.revision_note}`
    : state.text

  const result = await chain.invoke({ text: promptText })

  try {
    let content = (result.content as string).trim()
    // ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
    content = content.replace(/^```json\s*/i, '').replace(/\n?```\s*$/, '')
    const parsed = JSON.parse(content)
    state.questions = Array.isArray(parsed) ? parsed : [parsed]
    state.retry_count = (state.retry_count || 0) + 1
  }
  catch (e: any) {
    state.error = `é¢˜ç›®è§£æå¤±è´¥: ${e.message}`
    state.questions = []
  }

  return state
}

async function generateQuestions(state: WorkflowState): Promise<WorkflowState> {
  const nodeConfig = getNodeConfig(state, 'generate_questions')
  const llm = nodeConfig
    ? makeLLM(nodeConfig.modelInfo, nodeConfig.config)
    : makeLLM()

  const systemPrompt = `ä½ æ˜¯å‡ºé¢˜åŠ©æ‰‹ï¼Œæ ¹æ®ç¬”è®°å†…å®¹ç”Ÿæˆé¢˜ç›®ã€‚
  æ¯é“é¢˜å¿…é¡»åŒ…å«ï¼š
  - type: "single_choice" | "multiple_choice" | "true_false"
  - question: é¢˜ç›®å†…å®¹
  - options: é€‰é¡¹æ•°ç»„
  - answer: æ­£ç¡®ç­”æ¡ˆ
  - explanation: ç­”æ¡ˆè§£æ
  ${state.revision_note ? `\nç”¨æˆ·ä¿®æ”¹å»ºè®®ï¼š${state.revision_note}\nè¯·æ ¹æ®å»ºè®®è°ƒæ•´è¾“å‡ºã€‚` : ''}
  
  ç›´æ¥è¾“å‡ºJSONæ•°ç»„ã€‚`
  const prompt = ChatPromptTemplate.fromMessages([
    ['system', systemPrompt],
    ['human', '{text}'],
  ])

  const chain = prompt.pipe(llm)

  const result = await chain.invoke({
    text: state.text,
    num_questions: state.num_questions ?? 15,
  })

  try {
    let content = (result.content as string).trim()
    content = content.replace(/^```json\s*/i, '').replace(/\n?```\s*$/, '')
    const parsed = JSON.parse(content)
    state.questions = Array.isArray(parsed) ? parsed : [parsed]
    state.retry_count = (state.retry_count || 0) + 1
  }
  catch (e: any) {
    state.error = `é¢˜ç›®ç”Ÿæˆå¤±è´¥: ${e.message}`
    state.questions = []
  }

  return state
}

// ---------- 4. äººå·¥å®¡æ ¸ï¼ˆç­‰å¾…å‰ç«¯åé¦ˆï¼‰ ----------
const feedbackStore = new Map<string, HumanFeedbackInput>()

export function submitFeedback(workflowId: string, feedback: HumanFeedbackInput) {
  feedbackStore.set(workflowId, feedback)
}

async function waitForHumanFeedback(state: WorkflowState): Promise<WorkflowState> {
  const workflowId = state.file_path // ä½¿ç”¨æ–‡ä»¶è·¯å¾„ä½œä¸ºå”¯ä¸€ID
  const timeout = 90000 // 90ç§’
  const startTime = Date.now()

  // è½®è¯¢ç­‰å¾…åé¦ˆ
  while (Date.now() - startTime < timeout) {
    const feedback = feedbackStore.get(workflowId)
    if (feedback) {
      state.user_feedback = feedback.feedback
      state.revision_note = feedback.revision_note
      feedbackStore.delete(workflowId) // æ¸…é™¤å·²ä½¿ç”¨çš„åé¦ˆ
      return state
    }
    await new Promise(resolve => setTimeout(resolve, 1000)) // æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
  }

  // è¶…æ—¶é»˜è®¤æ¥å—
  state.user_feedback = 'Accept'
  return state
}

// ---------- 5. ä¿å­˜åˆ°æ–‡ä»¶ ----------
async function saveToFile(state: WorkflowState): Promise<WorkflowState> {
  try {
    const outputDir = process.env.OUTPUT_DIR || './output'
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
    const fileName = `quiz_${timestamp}.json`
    const filePath = join(outputDir, fileName)

    const output = {
      source: state.file_path,
      classification: state.classification,
      generated_at: new Date().toISOString(),
      questions: state.questions,
    }

    await writeFile(filePath, JSON.stringify(output, null, 2), 'utf-8')
    state.saved_path = filePath
  }
  catch (e) {
    state.error = `ä¿å­˜å¤±è´¥: ${e.message}`
  }

  return state
}

// ---------- 6. æ£€æŸ¥é”™è¯¯ ----------
// eslint-disable-next-line unused-imports/no-unused-vars
function checkError(state: WorkflowState): string {
  if (state.error)
    return 'error'

  return 'continue'
}

// ---------- 7. è·¯ç”±å‡½æ•° ----------
function routeAfterClassification(state: WorkflowState): string {
  if (state.classification === 'mixed' || state.error)
    return 'error'
  if (state.classification === 'question')
    return 'parse'
  if (state.classification === 'note')
    return 'generate'
  return 'error'
}

function routeAfterFeedback(state: WorkflowState): string {
  if (!state.user_feedback || state.user_feedback === 'Accept')
    return 'save'

  if (state.user_feedback === 'Reject' || state.user_feedback === 'Revise') {
    // é˜²æ­¢æ— é™å¾ªç¯
    if ((state.retry_count || 0) >= 5)
      return 'save'

    // æ ¹æ®åŸå§‹åˆ†ç±»å†³å®šè¿”å›å“ªä¸ªèŠ‚ç‚¹
    return state.classification === 'question' ? 'retry_parse' : 'retry_generate'
  }

  return 'save'
}

// ---------- 8. é”™è¯¯å¤„ç†èŠ‚ç‚¹ ----------
async function handleError(state: WorkflowState): Promise<WorkflowState> {
  return state
}

// ---------- æ„å»ºå·¥ä½œæµ ----------
export function buildWorkflow() {
  const workflow = new StateGraph<WorkflowState>({
    channels: {
      file_path: { value: '' },
      text: { value: '' },
      classification: { value: 'note' as ClassificationLabel },
      subject: { value: 'unknown' as Subject },
      questions: { value: [] as QuizItem[] },
      num_questions: { value: 15 },
      user_feedback: { value: undefined },
      revision_note: { value: undefined },
      error: { value: undefined },
      saved_path: { value: undefined },
      retry_count: { value: 0 },
      workflowConfig: { value: undefined },
    },
  })

  // æ·»åŠ èŠ‚ç‚¹
  workflow.addNode('load_file', loadFile)
  workflow.addNode('classify', classify)
  workflow.addNode('parse_questions', parseQuestions)
  workflow.addNode('generate_questions', generateQuestions)
  workflow.addNode('wait_feedback', waitForHumanFeedback)
  workflow.addNode('save_file', saveToFile)
  workflow.addNode('handle_error', handleError)

  // è®¾ç½®å…¥å£
  workflow.setEntrypoint('load_file')

  // åŠ è½½æ–‡ä»¶ -> åˆ†ç±»
  workflow.addEdge('load_file', 'classify')

  // åˆ†ç±»åçš„æ¡ä»¶è·¯ç”±
  workflow.addConditionalEdges(
    'classify',
    routeAfterClassification,
    {
      parse: 'parse_questions', // è·¯å¾„1ï¼šé¢˜ç›®è§£æ
      generate: 'generate_questions', // è·¯å¾„2ï¼šç¬”è®°ç”Ÿæˆ
      error: 'handle_error',
    },
  )

  // è§£æé¢˜ç›® -> ç­‰å¾…åé¦ˆ
  workflow.addEdge('parse_questions', 'wait_feedback')

  // ç”Ÿæˆé¢˜ç›® -> ç­‰å¾…åé¦ˆ
  workflow.addEdge('generate_questions', 'wait_feedback')

  // åé¦ˆåçš„æ¡ä»¶è·¯ç”±
  workflow.addConditionalEdges(
    'wait_feedback',
    routeAfterFeedback,
    {
      save: 'save_file',
      retry_parse: 'parse_questions', // å›åˆ°è·¯å¾„1
      retry_generate: 'generate_questions', // å›åˆ°è·¯å¾„2
    },
  )

  // ä¿å­˜æ–‡ä»¶ -> ç»“æŸ
  workflow.addEdge('save_file', END)

  // é”™è¯¯å¤„ç† -> ç»“æŸ
  workflow.addEdge('handle_error', END)

  return workflow.compile()
}

// ---------- è¿è¡Œå·¥ä½œæµ ----------
export async function runWorkflow(
  filePath: string,
  numQuestions?: number,
  workflowConfig?: WorkflowNodeConfig[],
): Promise<WorkflowState> {
  const app = buildWorkflow()
  const initState: WorkflowState = {
    file_path: filePath,
    text: '',
    classification: 'note',
    subject: 'unknown',
    questions: [],
    num_questions: numQuestions ?? 15,
    retry_count: 0,
    workflowConfig,
  }

  const finalState = await app.invoke(initState)

  return finalState
}

// ---------- åªæ‰§è¡Œåˆ†ç±» ----------
export async function classifyFile(filePath: string): Promise<{
  classification: string
  error?: string
}> {
  console.log('ğŸ¯ [å·¥ä½œæµ] å¼€å§‹åˆ†ç±»æ–‡ä»¶:', filePath)

  try {
    const state: WorkflowState = {
      file_path: filePath,
      text: '',
      classification: 'note',
      subject: 'unknown',
      questions: [],
      num_questions: 0,
      retry_count: 0,
    }

    console.log('ğŸ“‚ [å·¥ä½œæµ] æ­¥éª¤ 1: åŠ è½½æ–‡ä»¶...')
    // åŠ è½½æ–‡ä»¶
    await loadFile(state)
    console.log('âœ… [å·¥ä½œæµ] æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦:', state.text.length)

    console.log('ğŸ” [å·¥ä½œæµ] æ­¥éª¤ 2: æ‰§è¡Œåˆ†ç±»...')
    // æ‰§è¡Œåˆ†ç±»
    await classify(state)
    console.log('âœ… [å·¥ä½œæµ] åˆ†ç±»å®Œæˆ:', {
      classification: state.classification,
      error: state.error,
    })

    return {
      classification: state.classification,
      error: state.error,
    }
  }
  catch (error: any) {
    console.error('âŒ [å·¥ä½œæµ] åˆ†ç±»å¤±è´¥:', error)
    console.error('é”™è¯¯è¯¦æƒ…:', {
      message: error?.message,
      stack: error?.stack,
      type: typeof error,
    })
    return {
      classification: 'unknown',
      error: error?.message || String(error),
    }
  }
}

// ---------- ä»ç¬”è®°ç”Ÿæˆé¢˜ç›®ï¼ˆæŒ‡å®šé¢˜å‹å’Œæ•°é‡ï¼‰ ----------
export async function generateQuestionsFromNote(
  filePath: string,
  questionTypes: { single_choice: number, multiple_choice: number, true_false: number },
): Promise<WorkflowState & { scoreDistribution?: any }> {
  try {
    const state: WorkflowState = {
      file_path: filePath,
      text: '',
      classification: 'note',
      subject: 'unknown',
      questions: [],
      num_questions: 0,
      retry_count: 0,
    }

    // åŠ è½½æ–‡ä»¶
    await loadFile(state)

    // æ„å»ºæç¤ºè¯
    const totalQuestions
      = questionTypes.single_choice + questionTypes.multiple_choice + questionTypes.true_false

    const llm = makeLLM()
    const systemPrompt = `ä½ æ˜¯å‡ºé¢˜åŠ©æ‰‹ï¼Œæ ¹æ®ç¬”è®°å†…å®¹ç”Ÿæˆé¢˜ç›®ï¼Œå¹¶ä¸ºæ¯ç§é¢˜å‹åˆ†é…åˆ†æ•°ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ç”Ÿæˆï¼š
- å•é€‰é¢˜ï¼š${questionTypes.single_choice}é“
- å¤šé€‰é¢˜ï¼š${questionTypes.multiple_choice}é“
- åˆ¤æ–­é¢˜ï¼š${questionTypes.true_false}é“

è¯·æ ¹æ®é¢˜ç›®éš¾åº¦åˆç†åˆ†é…åˆ†æ•°ï¼Œä¸€èˆ¬æ¥è¯´ï¼š
- å•é€‰é¢˜æ¯é¢˜å»ºè®® 3-5 åˆ†
- å¤šé€‰é¢˜æ¯é¢˜å»ºè®® 5-8 åˆ†ï¼ˆéš¾åº¦è¾ƒé«˜ï¼‰
- åˆ¤æ–­é¢˜æ¯é¢˜å»ºè®® 2-3 åˆ†

æ¯é“é¢˜å¿…é¡»åŒ…å«ï¼š
- type: "single_choice" | "multiple_choice" | "true_false"
- question: é¢˜ç›®å†…å®¹
- options: é€‰é¡¹æ•°ç»„ï¼ˆåˆ¤æ–­é¢˜ä¸º ["æ­£ç¡®", "é”™è¯¯"]ï¼‰
- answer: æ­£ç¡®ç­”æ¡ˆï¼ˆæ•°ç»„å½¢å¼ï¼Œå¦‚ ["A"] æˆ– ["A","B"]ï¼‰
- explanation: ç­”æ¡ˆè§£æ
- score: è¯¥é¢˜åˆ†æ•°ï¼ˆæ•´æ•°ï¼‰

è¿”å›æ ¼å¼ï¼š
{{
  "questions": [...é¢˜ç›®æ•°ç»„...],
  "scoreDistribution": {{
    "single_choice": {{ "perQuestion": æ¯é¢˜åˆ†æ•°, "total": å•é€‰é¢˜æ€»åˆ† }},
    "multiple_choice": {{ "perQuestion": æ¯é¢˜åˆ†æ•°, "total": å¤šé€‰é¢˜æ€»åˆ† }},
    "true_false": {{ "perQuestion": æ¯é¢˜åˆ†æ•°, "total": åˆ¤æ–­é¢˜æ€»åˆ† }}
  }}
}}

ç›´æ¥è¾“å‡ºJSONå¯¹è±¡ï¼Œä¸è¦ä»»ä½•é¢å¤–è¯´æ˜ã€‚`

    const prompt = ChatPromptTemplate.fromMessages([
      ['system', systemPrompt],
      ['human', 'è¯·æ ¹æ®ä»¥ä¸‹ç¬”è®°ç”Ÿæˆé¢˜ç›®:\n\n{text}'],
    ])

    const chain = prompt.pipe(llm)
    const result = await chain.invoke({ text: state.text })

    let content = (result.content as string).trim()
    content = content.replace(/^```json\s*/i, '').replace(/\n?```\s*$/, '')
    const parsed = JSON.parse(content)

    // å…¼å®¹æ—§æ ¼å¼ï¼ˆç›´æ¥è¿”å›æ•°ç»„ï¼‰å’Œæ–°æ ¼å¼ï¼ˆè¿”å›å¯¹è±¡ï¼‰
    if (Array.isArray(parsed)) {
      state.questions = parsed
      state.num_questions = totalQuestions
      return state
    }
    else {
      state.questions = parsed.questions || []
      state.num_questions = totalQuestions
      return {
        ...state,
        scoreDistribution: parsed.scoreDistribution,
      }
    }
  }
  catch (error: any) {
    throw new Error(`ç”Ÿæˆé¢˜ç›®å¤±è´¥: ${error?.message || String(error)}`)
  }
}

// ---------- æµ‹è¯• LLM è¿æ¥ ----------
export async function testLLMConnection(): Promise<{
  success: boolean
  message: string
  model?: string
  response?: string
}> {
  console.log('ğŸ§ª [æµ‹è¯•] å¼€å§‹æµ‹è¯• LLM è¿æ¥...')

  try {
    // åˆ›å»º LLM å®ä¾‹
    const llm = makeLLM()
    console.log('âœ… [æµ‹è¯•] LLM å®ä¾‹åˆ›å»ºæˆåŠŸ')

    // å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•é—®é¢˜
    const prompt = ChatPromptTemplate.fromMessages([
      ['system', 'ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚è¯·ç”¨ä¸€å¥è¯å›ç­”é—®é¢˜ã€‚'],
      ['human', 'è¯·è¯´"ä½ å¥½ï¼ŒLLM è¿æ¥æˆåŠŸï¼"'],
    ])

    const chain = prompt.pipe(llm)
    console.log('ğŸ”„ [æµ‹è¯•] æ­£åœ¨å‘é€æµ‹è¯•è¯·æ±‚...')

    const result = await chain.invoke({ text: 'æµ‹è¯•' })
    const response = (result.content as string).trim()

    console.log('âœ… [æµ‹è¯•] LLM å“åº”æˆåŠŸ!')
    console.log('ğŸ“ [æµ‹è¯•] å“åº”å†…å®¹:', response)

    return {
      success: true,
      message: 'LLM è¿æ¥æµ‹è¯•æˆåŠŸï¼',
      model: process.env.OPENAI_API_MODEL || 'gpt-4o-mini',
      response,
    }
  }
  catch (error: any) {
    console.error('âŒ [æµ‹è¯•] LLM è¿æ¥å¤±è´¥:', error)
    console.error('é”™è¯¯è¯¦æƒ…:', {
      name: error?.name,
      message: error?.message,
      code: error?.code,
      status: error?.status,
      response: error?.response?.data,
    })

    return {
      success: false,
      message: `LLM è¿æ¥å¤±è´¥: ${error?.message || String(error)}`,
    }
  }
}
