import type { ClassificationLabel, HumanFeedbackInput, ModelConfig, ModelInfo, QuizItem, Subject, WorkflowNodeConfig, WorkflowNodeType, WorkflowState } from './types'
// workflow.ts
import { writeFile } from 'node:fs/promises'
import { join } from 'node:path'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { END, StateGraph } from '@langchain/langgraph'
import { ChatOpenAI } from '@langchain/openai'
import {
  DEFAULT_CLASSIFIER_PROMPT,
  DEFAULT_GENERATE_QUESTIONS_PROMPT,
  DEFAULT_PARSE_QUESTIONS_PROMPT,
  DEFAULT_REVIEW_AND_SCORE_PROMPT,
  getDefaultGenerateQuestionsWithTypesPrompt,
} from './defaultPrompts'
import { loadFile } from './loader'

// ---------- LLM ----------
// åˆ¤æ–­æ¨¡å‹æ˜¯å¦ä¸º Kriora ä¾›åº”å•†
function isKrioraModel(modelId: string): boolean {
  return modelId.includes('moonshotai/') || modelId.includes('qwen/')
}

function makeLLM(modelInfo?: ModelInfo, config?: ModelConfig) {
  // å¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹ä¿¡æ¯ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
  const model = modelInfo?.name || process.env.OPENAI_API_MODEL || 'gpt-4o-mini'

  // æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„ API é…ç½®
  let apiKey = modelInfo?.apiKey
  let baseURL = modelInfo?.baseURL

  if (!apiKey || !baseURL) {
    if (isKrioraModel(model)) {
      // ä½¿ç”¨ Kriora API é…ç½®
      apiKey = apiKey || process.env.KRIORA_API_KEY || process.env.OPENAI_API_KEY
      baseURL = baseURL || process.env.KRIORA_API_URL || 'https://api.kriora.com'
    }
    else {
      // ä½¿ç”¨é»˜è®¤ OpenAI API é…ç½®
      apiKey = apiKey || process.env.OPENAI_API_KEY
      baseURL = baseURL || process.env.OPENAI_API_BASE_URL
    }
  }

  console.warn('ğŸ”‘ [LLMé…ç½®]', {
    model,
    baseURL,
    hasApiKey: !!apiKey,
    provider: isKrioraModel(model) ? 'kriora' : (modelInfo?.provider || 'openai'),
    config,
  })

  if (!apiKey)
    throw new Error('API_KEY æœªé…ç½®ï¼è¯·åœ¨ service/.env æ–‡ä»¶ä¸­é…ç½®æˆ–é€šè¿‡å·¥ä½œæµé…ç½®ä¼ å…¥')

  return new ChatOpenAI({
    model,
    temperature: config?.temperature ?? 0,
    topP: config?.top_p,
    maxTokens: config?.max_tokens,
    presencePenalty: config?.presence_penalty,
    frequencyPenalty: config?.frequency_penalty,
    openAIApiKey: apiKey,
    configuration: {
      // æ¨¡å‹è°ƒç”¨éœ€è¦åŠ  /v1
      baseURL: baseURL ? `${baseURL}/v1` : 'https://api.openai.com/v1',
    },
    streaming: true,
    timeout: 60000,
  })
}

// è·å–èŠ‚ç‚¹é…ç½®
function getNodeConfig(state: WorkflowState, nodeType: WorkflowNodeType): { modelInfo: ModelInfo, config: ModelConfig } | null {
  const nodeConfig = state.workflowConfig?.find(c => c.nodeType === nodeType)
  if (!nodeConfig)
    return null

  // å¦‚æœæœ‰å­¦ç§‘ä¿¡æ¯ä¸”è¯¥èŠ‚ç‚¹æœ‰å­¦ç§‘ä¸“å±é…ç½®ï¼Œä½¿ç”¨å­¦ç§‘ä¸“å±æ¨¡å‹
  if (state.subject && state.subject !== 'unknown' && nodeConfig.subjectSpecific?.[state.subject]) {
    return {
      modelInfo: nodeConfig.subjectSpecific[state.subject]!,
      config: nodeConfig.config || {},
    }
  }

  return {
    modelInfo: nodeConfig.modelInfo,
    config: nodeConfig.config || {},
  }
}

// ---------- 1. åˆ†ç±»å™¨ï¼ˆå¢å¼ºï¼šåŒæ—¶è¯†åˆ«å­¦ç§‘ï¼‰ ----------
async function classify(state: WorkflowState): Promise<WorkflowState> {
  console.warn('ğŸ¤– [åˆ†ç±»å™¨] å¼€å§‹è°ƒç”¨ LLM è¿›è¡Œåˆ†ç±»...')
  console.warn('ğŸ“ [åˆ†ç±»å™¨] æ–‡æœ¬é¢„è§ˆ (å‰100å­—):', state.text.slice(0, 100))

  try {
    const nodeConfig = getNodeConfig(state, 'classify')
    const llm = nodeConfig
      ? makeLLM(nodeConfig.modelInfo, nodeConfig.config)
      : makeLLM()

    // ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®çš„æç¤ºè¯ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯
    const systemPrompt = nodeConfig?.systemPrompt || DEFAULT_CLASSIFIER_PROMPT
    const classifierPrompt = ChatPromptTemplate.fromMessages([
      ['system', systemPrompt],
      ['human', '{text}'],
    ])

    const chain = classifierPrompt.pipe(llm)
    const textSample = state.text.slice(0, 3000)

    console.warn('ğŸ”„ [åˆ†ç±»å™¨] å‘é€æ–‡æœ¬ç»™ LLMï¼Œé•¿åº¦:', textSample.length)

    const result = await chain.invoke({ text: textSample })
    const response = (result.content as string).trim().toLowerCase()

    console.warn('ğŸ“Š [åˆ†ç±»å™¨] LLM è¿”å›ç»“æœ:', response)

    // è§£æå“åº”
    const typeMatch = response.match(/type:\s*(note|question|mixed|unknown)/)
    const subjectMatch = response.match(/subject:\s*(math|physics|chemistry|biology|chinese|english|unknown)/)

    const type = typeMatch?.[1] || 'unknown'
    const subject = subjectMatch?.[1] || 'unknown'

    state.classification = type as ClassificationLabel
    state.subject = subject as Subject

    console.warn('âœ… [åˆ†ç±»å™¨] åˆ†ç±»ç»“æœ:', {
      type: state.classification,
      subject: state.subject,
    })

    if (state.classification === 'mixed')
      state.error = 'æ–‡ä»¶åŒæ—¶åŒ…å«ç¬”è®°å’Œé¢˜ç›®ï¼Œè¯·åˆ†å¼€å¤„ç†'
  }
  catch (error: any) {
    console.error('âŒ [åˆ†ç±»å™¨] API è°ƒç”¨å¤±è´¥:', error)
    console.error('é”™è¯¯è¯¦æƒ…:', {
      name: error?.name,
      message: error?.message,
      code: error?.code,
      status: error?.status,
      response: error?.response?.data,
    })

    state.classification = 'note'
    state.subject = 'unknown'
    state.error = `API è°ƒç”¨å¤±è´¥: ${error?.message || String(error)}`

    throw error
  }

  return state
}

async function parseQuestions(state: WorkflowState): Promise<WorkflowState> {
  const nodeConfig = getNodeConfig(state, 'parse_questions')
  const llm = nodeConfig
    ? makeLLM(nodeConfig.modelInfo, nodeConfig.config)
    : makeLLM()

  // ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®çš„æç¤ºè¯ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯
  const basePrompt = nodeConfig?.systemPrompt || DEFAULT_PARSE_QUESTIONS_PROMPT

  // åŠ¨æ€æ‹¼æ¥ç”¨æˆ·ä¿®æ”¹å»ºè®®ï¼ˆå¦‚æœæœ‰ï¼‰
  const systemPrompt = state.revision_note
    ? `${basePrompt}\n\n### ç”¨æˆ·ä¿®æ”¹å»ºè®®\n\`\`\`\n${state.revision_note}\n\`\`\`\n\nè¯·æ ¹æ®ç”¨æˆ·çš„ä¿®æ”¹å»ºè®®è°ƒæ•´è§£æç»“æœï¼š\n- å¦‚æœå»ºè®®ä¿®æ­£ç­”æ¡ˆï¼Œè¯·æ ¸å®å¹¶ä¿®æ”¹\n- å¦‚æœå»ºè®®è°ƒæ•´é€‰é¡¹ï¼Œè¯·ç›¸åº”ä¿®æ”¹\n- å¦‚æœå»ºè®®åˆ é™¤æŸäº›é¢˜ç›®ï¼Œè¯·è¿‡æ»¤æ‰\n- åœ¨ explanation ä¸­è¯´æ˜ä¿®æ”¹å†…å®¹`
    : basePrompt

  const prompt = ChatPromptTemplate.fromMessages([
    ['system', systemPrompt],
    ['human', '{text}'],
  ])

  const chain = prompt.pipe(llm)

  const promptText = state.revision_note
    ? `åŸå§‹é¢˜ç›®ï¼š\n${state.text}\n\nä¹‹å‰çš„è§£æç»“æœï¼š\n${JSON.stringify(state.questions, null, 2)}\n\nä¿®æ”¹å»ºè®®ï¼š${state.revision_note}`
    : state.text

  const result = await chain.invoke({ text: promptText })

  try {
    let content = (result.content as string).trim()
    // ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
    content = content.replace(/^```json\s*/i, '').replace(/\n?```\s*$/, '')
    const parsed = JSON.parse(content)
    state.questions = Array.isArray(parsed) ? parsed : [parsed]
    state.retry_count = (state.retry_count || 0) + 1
  }
  catch (e: any) {
    state.error = `é¢˜ç›®è§£æå¤±è´¥: ${e.message}`
    state.questions = []
  }

  return state
}

async function generateQuestions(state: WorkflowState): Promise<WorkflowState> {
  const nodeConfig = getNodeConfig(state, 'generate_questions')
  const llm = nodeConfig
    ? makeLLM(nodeConfig.modelInfo, nodeConfig.config)
    : makeLLM()

  // ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®çš„æç¤ºè¯ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯
  const basePrompt = nodeConfig?.systemPrompt || DEFAULT_GENERATE_QUESTIONS_PROMPT

  // åŠ¨æ€æ‹¼æ¥ç”¨æˆ·ä¿®æ”¹å»ºè®®ï¼ˆå¦‚æœæœ‰ï¼‰
  const systemPrompt = state.revision_note
    ? `${basePrompt}\n\n## ç”¨æˆ·ä¿®æ”¹å»ºè®®å¤„ç†\n\n### ç”¨æˆ·ä¿®æ”¹å»ºè®®\n\`\`\`\n${state.revision_note}\n\`\`\`\n\nè¯·æ ¹æ®ç”¨æˆ·çš„ä¿®æ”¹å»ºè®®è°ƒæ•´é¢˜ç›®ç”Ÿæˆï¼š\n- å¦‚æœå»ºè®®è°ƒæ•´éš¾åº¦ï¼Œè¯·ç›¸åº”ä¿®æ”¹é¢˜ç›®å¤æ‚åº¦\n- å¦‚æœå»ºè®®å¢åŠ æŸä¸ªçŸ¥è¯†ç‚¹ï¼Œè¯·å¢åŠ ç›¸å…³é¢˜ç›®\n- å¦‚æœå»ºè®®ä¿®æ”¹é¢˜å‹æ¯”ä¾‹ï¼Œè¯·è°ƒæ•´é¢˜å‹åˆ†é…\n- å¦‚æœå»ºè®®ä¿®æ”¹å…·ä½“é¢˜ç›®ï¼Œè¯·é’ˆå¯¹æ€§ä¿®æ”¹`
    : basePrompt

  const prompt = ChatPromptTemplate.fromMessages([
    ['system', systemPrompt],
    ['human', '{text}'],
  ])

  const chain = prompt.pipe(llm)

  const result = await chain.invoke({
    text: state.text,
    num_questions: state.num_questions ?? 15,
  })

  try {
    let content = (result.content as string).trim()
    content = content.replace(/^```json\s*/i, '').replace(/\n?```\s*$/, '')
    const parsed = JSON.parse(content)
    state.questions = Array.isArray(parsed) ? parsed : [parsed]
    state.retry_count = (state.retry_count || 0) + 1
  }
  catch (e: any) {
    state.error = `é¢˜ç›®ç”Ÿæˆå¤±è´¥: ${e.message}`
    state.questions = []
  }

  return state
}

// ---------- 4. å®¡æ ¸ä¸“å®¶AIï¼ˆå®¡æ ¸è´¨é‡å¹¶åˆ†é…åˆ†æ•°ï¼‰ ----------
async function reviewAndScore(state: WorkflowState): Promise<WorkflowState> {
  console.warn('ğŸ” [å®¡æ ¸ä¸“å®¶] å¼€å§‹å®¡æ ¸é¢˜ç›®è´¨é‡å¹¶åˆ†é…åˆ†æ•°...')

  try {
    const nodeConfig = getNodeConfig(state, 'review_and_score')
    const llm = nodeConfig
      ? makeLLM(nodeConfig.modelInfo, nodeConfig.config)
      : makeLLM()

    // ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®çš„æç¤ºè¯ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯
    const systemPrompt = nodeConfig?.systemPrompt || DEFAULT_REVIEW_AND_SCORE_PROMPT

    const prompt = ChatPromptTemplate.fromMessages([
      ['system', systemPrompt],
      ['human', 'è¯·å®¡æ ¸ä»¥ä¸‹é¢˜ç›®å¹¶åˆ†é…åˆ†æ•°:\n\n{questions}'],
    ])

    const chain = prompt.pipe(llm)

    // å°†é¢˜ç›®è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ä¼ é€’ç»™ LLM
    const questionsJson = JSON.stringify(state.questions, null, 2)

    const result = await chain.invoke({
      questions: questionsJson,
    })

    // è§£æç»“æœ
    let content = (result.content as string).trim()
    content = content.replace(/^```json\s*/i, '').replace(/\n?```\s*$/, '')
    const parsed = JSON.parse(content)

    // æ›´æ–°çŠ¶æ€
    if (parsed.questions && Array.isArray(parsed.questions)) {
      state.questions = parsed.questions
      console.warn('âœ… [å®¡æ ¸ä¸“å®¶] å®¡æ ¸å®Œæˆï¼Œå·²åˆ†é…åˆ†æ•°')

      // éªŒè¯æ€»åˆ†
      if (parsed.scoreDistribution) {
        const totalScore = parsed.scoreDistribution.totalScore
        console.warn(`ğŸ“Š [å®¡æ ¸ä¸“å®¶] åˆ†æ•°ç»Ÿè®¡: æ€»åˆ†=${totalScore}`)

        if (totalScore !== 100) {
          console.warn(`âš ï¸ [å®¡æ ¸ä¸“å®¶] è­¦å‘Šï¼šæ€»åˆ†ä¸æ˜¯100åˆ†ï¼ˆå®é™…ä¸º${totalScore}åˆ†ï¼‰`)
        }
      }
    }
    else {
      throw new Error('å®¡æ ¸ç»“æœæ ¼å¼é”™è¯¯ï¼šç¼ºå°‘ questions æ•°ç»„')
    }

    state.retry_count = (state.retry_count || 0) + 1
  }
  catch (e: any) {
    console.error('âŒ [å®¡æ ¸ä¸“å®¶] å®¡æ ¸å¤±è´¥:', e.message)
    state.error = `é¢˜ç›®å®¡æ ¸å¤±è´¥: ${e.message}`
    state.questions = []
  }

  return state
}

// ---------- 5. äººå·¥å®¡æ ¸ï¼ˆç­‰å¾…å‰ç«¯åé¦ˆï¼‰ ----------
const feedbackStore = new Map<string, HumanFeedbackInput>()

export function submitFeedback(workflowId: string, feedback: HumanFeedbackInput) {
  feedbackStore.set(workflowId, feedback)
}

async function waitForHumanFeedback(state: WorkflowState): Promise<WorkflowState> {
  const workflowId = state.file_path // ä½¿ç”¨æ–‡ä»¶è·¯å¾„ä½œä¸ºå”¯ä¸€ID
  const timeout = 90000 // 90ç§’
  const startTime = Date.now()

  // è½®è¯¢ç­‰å¾…åé¦ˆ
  while (Date.now() - startTime < timeout) {
    const feedback = feedbackStore.get(workflowId)
    if (feedback) {
      state.user_feedback = feedback.feedback
      state.revision_note = feedback.revision_note
      feedbackStore.delete(workflowId) // æ¸…é™¤å·²ä½¿ç”¨çš„åé¦ˆ
      return state
    }
    await new Promise(resolve => setTimeout(resolve, 1000)) // æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
  }

  // è¶…æ—¶é»˜è®¤æ¥å—
  state.user_feedback = 'Accept'
  return state
}

// ---------- 5. ä¿å­˜åˆ°æ–‡ä»¶ ----------
async function saveToFile(state: WorkflowState): Promise<WorkflowState> {
  try {
    const outputDir = process.env.OUTPUT_DIR || './output'
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
    const fileName = `quiz_${timestamp}.json`
    const filePath = join(outputDir, fileName)

    const output = {
      source: state.file_path,
      classification: state.classification,
      generated_at: new Date().toISOString(),
      questions: state.questions,
    }

    await writeFile(filePath, JSON.stringify(output, null, 2), 'utf-8')
    state.saved_path = filePath
  }
  catch (e) {
    state.error = `ä¿å­˜å¤±è´¥: ${e.message}`
  }

  return state
}

// ---------- 6. æ£€æŸ¥é”™è¯¯ ----------
// eslint-disable-next-line unused-imports/no-unused-vars
function checkError(state: WorkflowState): string {
  if (state.error)
    return 'error'

  return 'continue'
}

// ---------- 7. è·¯ç”±å‡½æ•° ----------
function routeAfterClassification(state: WorkflowState): string {
  if (state.classification === 'mixed' || state.error)
    return 'error'
  if (state.classification === 'question')
    return 'parse'
  if (state.classification === 'note')
    return 'generate'
  return 'error'
}

function routeAfterFeedback(state: WorkflowState): string {
  if (!state.user_feedback || state.user_feedback === 'Accept')
    return 'save'

  if (state.user_feedback === 'Reject' || state.user_feedback === 'Revise') {
    // é˜²æ­¢æ— é™å¾ªç¯
    if ((state.retry_count || 0) >= 5)
      return 'save'

    // æ ¹æ®åŸå§‹åˆ†ç±»å†³å®šè¿”å›å“ªä¸ªèŠ‚ç‚¹
    return state.classification === 'question' ? 'retry_parse' : 'retry_generate'
  }

  return 'save'
}

// ---------- 8. é”™è¯¯å¤„ç†èŠ‚ç‚¹ ----------
async function handleError(state: WorkflowState): Promise<WorkflowState> {
  return state
}

// ---------- æ„å»ºå·¥ä½œæµ ----------
export function buildWorkflow() {
  const workflow = new StateGraph<WorkflowState>({
    channels: {
      file_path: { value: '' },
      text: { value: '' },
      classification: { value: 'note' as ClassificationLabel },
      subject: { value: 'unknown' as Subject },
      questions: { value: [] as QuizItem[] },
      num_questions: { value: 15 },
      user_feedback: { value: undefined },
      revision_note: { value: undefined },
      error: { value: undefined },
      saved_path: { value: undefined },
      retry_count: { value: 0 },
      workflowConfig: { value: undefined },
    },
  })

  // æ·»åŠ èŠ‚ç‚¹
  workflow.addNode('load_file', loadFile)
  workflow.addNode('classify', classify)
  workflow.addNode('parse_questions', parseQuestions)
  workflow.addNode('generate_questions', generateQuestions)
  workflow.addNode('review_and_score', reviewAndScore)
  workflow.addNode('wait_feedback', waitForHumanFeedback)
  workflow.addNode('save_file', saveToFile)
  workflow.addNode('handle_error', handleError)

  // è®¾ç½®å…¥å£
  workflow.setEntrypoint('load_file')

  // åŠ è½½æ–‡ä»¶ -> åˆ†ç±»
  workflow.addEdge('load_file', 'classify')

  // åˆ†ç±»åçš„æ¡ä»¶è·¯ç”±
  workflow.addConditionalEdges(
    'classify',
    routeAfterClassification,
    {
      parse: 'parse_questions', // è·¯å¾„1ï¼šé¢˜ç›®è§£æ
      generate: 'generate_questions', // è·¯å¾„2ï¼šç¬”è®°ç”Ÿæˆ
      error: 'handle_error',
    },
  )

  // è§£æé¢˜ç›® -> å®¡æ ¸å¹¶åˆ†é…åˆ†æ•°
  workflow.addEdge('parse_questions', 'review_and_score')

  // ç”Ÿæˆé¢˜ç›® -> å®¡æ ¸å¹¶åˆ†é…åˆ†æ•°
  workflow.addEdge('generate_questions', 'review_and_score')

  // å®¡æ ¸å®Œæˆ -> ç­‰å¾…åé¦ˆ
  workflow.addEdge('review_and_score', 'wait_feedback')

  // åé¦ˆåçš„æ¡ä»¶è·¯ç”±
  workflow.addConditionalEdges(
    'wait_feedback',
    routeAfterFeedback,
    {
      save: 'save_file',
      retry_parse: 'parse_questions', // å›åˆ°è·¯å¾„1
      retry_generate: 'generate_questions', // å›åˆ°è·¯å¾„2
    },
  )

  // ä¿å­˜æ–‡ä»¶ -> ç»“æŸ
  workflow.addEdge('save_file', END)

  // é”™è¯¯å¤„ç† -> ç»“æŸ
  workflow.addEdge('handle_error', END)

  return workflow.compile()
}

// ---------- è¿è¡Œå·¥ä½œæµ ----------
export async function runWorkflow(
  filePath: string,
  numQuestions?: number,
  workflowConfig?: WorkflowNodeConfig[],
): Promise<WorkflowState> {
  const app = buildWorkflow()
  const initState: WorkflowState = {
    file_path: filePath,
    text: '',
    classification: 'note',
    subject: 'unknown',
    questions: [],
    num_questions: numQuestions ?? 15,
    retry_count: 0,
    workflowConfig,
  }

  const finalState = await app.invoke(initState)

  return finalState
}

// ---------- åªæ‰§è¡Œåˆ†ç±» ----------
export async function classifyFile(
  filePath: string,
  workflowConfig?: WorkflowNodeConfig[],
): Promise<{
  classification: string
  error?: string
}> {
  console.warn('ğŸ¯ [å·¥ä½œæµ] å¼€å§‹åˆ†ç±»æ–‡ä»¶:', filePath)

  try {
    const state: WorkflowState = {
      file_path: filePath,
      text: '',
      classification: 'note',
      subject: 'unknown',
      questions: [],
      num_questions: 0,
      retry_count: 0,
      workflowConfig,
    }

    console.warn('ğŸ“‚ [å·¥ä½œæµ] æ­¥éª¤ 1: åŠ è½½æ–‡ä»¶...')
    // åŠ è½½æ–‡ä»¶
    await loadFile(state)
    console.warn('âœ… [å·¥ä½œæµ] æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦:', state.text.length)

    console.warn('ğŸ” [å·¥ä½œæµ] æ­¥éª¤ 2: æ‰§è¡Œåˆ†ç±»...')
    // æ‰§è¡Œåˆ†ç±»
    await classify(state)
    console.warn('âœ… [å·¥ä½œæµ] åˆ†ç±»å®Œæˆ:', {
      classification: state.classification,
      error: state.error,
    })

    return {
      classification: state.classification,
      error: state.error,
    }
  }
  catch (error: any) {
    console.error('âŒ [å·¥ä½œæµ] åˆ†ç±»å¤±è´¥:', error)
    console.error('é”™è¯¯è¯¦æƒ…:', {
      message: error?.message,
      stack: error?.stack,
      type: typeof error,
    })
    return {
      classification: 'unknown',
      error: error?.message || String(error),
    }
  }
}

// ---------- ä»ç¬”è®°ç”Ÿæˆé¢˜ç›®ï¼ˆæŒ‡å®šé¢˜å‹å’Œæ•°é‡ï¼‰ ----------
export async function generateQuestionsFromNote(
  filePath: string,
  questionTypes: { single_choice: number, multiple_choice: number, true_false: number },
  workflowConfig?: WorkflowNodeConfig[],
  progressManager?: ReturnType<typeof import('./workflowProgressManager').createWorkflowProgressManager>,
): Promise<WorkflowState & { scoreDistribution?: any }> {
  try {
    const state: WorkflowState = {
      file_path: filePath,
      text: '',
      classification: 'note',
      subject: 'unknown',
      questions: [],
      num_questions: 0,
      retry_count: 0,
      workflowConfig,
    }

    // åŠ è½½æ–‡ä»¶
    progressManager?.updateNodeStatus('generate_questions', 'running', 'æ­£åœ¨åŠ è½½æ–‡ä»¶...')
    await loadFile(state)

    // è·å–èŠ‚ç‚¹é…ç½®
    const nodeConfig = getNodeConfig(state, 'generate_questions')

    // æ„å»ºæç¤ºè¯
    const totalQuestions
      = questionTypes.single_choice + questionTypes.multiple_choice + questionTypes.true_false

    const llm = nodeConfig
      ? makeLLM(nodeConfig.modelInfo, nodeConfig.config)
      : makeLLM()

    // ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®çš„æç¤ºè¯ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯
    const systemPrompt = nodeConfig?.systemPrompt || getDefaultGenerateQuestionsWithTypesPrompt(questionTypes)

    const prompt = ChatPromptTemplate.fromMessages([
      ['system', systemPrompt],
      ['human', 'è¯·æ ¹æ®ä»¥ä¸‹ç¬”è®°ç”Ÿæˆé¢˜ç›®:\n\n{text}'],
    ])

    const chain = prompt.pipe(llm)
    progressManager?.updateNodeStatus('generate_questions', 'running', `æ­£åœ¨ç”Ÿæˆ ${totalQuestions} é“é¢˜ç›®...`)
    const result = await chain.invoke({ text: state.text })

    let content = (result.content as string).trim()
    content = content.replace(/^```json\s*/i, '').replace(/\n?```\s*$/, '')
    const parsed = JSON.parse(content)

    // è®¾ç½®ç”Ÿæˆçš„é¢˜ç›®åˆ° stateï¼ˆé¢˜ç›®æ­¤æ—¶æ²¡æœ‰åˆ†æ•°ï¼‰
    state.questions = Array.isArray(parsed) ? parsed : (parsed.questions || [])
    state.num_questions = totalQuestions

    console.warn('ğŸ“ [é¢˜ç›®ç”Ÿæˆ] ç”Ÿæˆå®Œæˆï¼Œå…±', state.questions.length, 'é¢˜ï¼Œå¼€å§‹å®¡æ ¸...')
    progressManager?.updateNodeStatus(
      'generate_questions',
      'completed',
      `é¢˜ç›®ç”Ÿæˆå®Œæˆï¼Œå…± ${state.questions.length} é¢˜`,
      { questions: state.questions }, // ğŸ”¥ ä¼ é€’é¢˜ç›®æ•°æ®ç»™å‰ç«¯
    )

    // ğŸ”¥ è°ƒç”¨å®¡æ ¸ä¸“å®¶AIåˆ†é…åˆ†æ•°
    progressManager?.updateNodeStatus('review_and_score', 'running', 'æ­£åœ¨å®¡æ ¸å¹¶åˆ†é…åˆ†æ•°...')
    await reviewAndScore(state)

    if (state.error) {
      throw new Error(state.error)
    }

    console.warn('âœ… [é¢˜ç›®ç”Ÿæˆ] å®¡æ ¸å®Œæˆï¼Œå·²åˆ†é…åˆ†æ•°')

    // ğŸ”¥ è®¡ç®—åˆ†æ•°ç»Ÿè®¡ï¼ˆæå‰è®¡ç®—ï¼Œç”¨äºä¼ é€’ç»™å‰ç«¯ï¼‰
    const tempScoreDistribution = {
      single_choice: { perQuestion: 0, total: 0 },
      multiple_choice: { perQuestion: 0, total: 0 },
      true_false: { perQuestion: 0, total: 0 },
      totalScore: 0,
    }

    state.questions.forEach((q) => {
      const score = q.score || 0
      if (q.type === 'single_choice') {
        tempScoreDistribution.single_choice.total += score
        if (tempScoreDistribution.single_choice.perQuestion === 0)
          tempScoreDistribution.single_choice.perQuestion = score
      }
      else if (q.type === 'multiple_choice') {
        tempScoreDistribution.multiple_choice.total += score
      }
      else if (q.type === 'true_false') {
        tempScoreDistribution.true_false.total += score
        if (tempScoreDistribution.true_false.perQuestion === 0)
          tempScoreDistribution.true_false.perQuestion = score
      }
      tempScoreDistribution.totalScore += score
    })

    progressManager?.updateNodeStatus(
      'review_and_score',
      'completed',
      'å®¡æ ¸æ‰“åˆ†å®Œæˆ',
      { questions: state.questions, scoreDistribution: tempScoreDistribution }, // ğŸ”¥ ä¼ é€’é¢˜ç›®å’Œåˆ†æ•°æ•°æ®
    )

    // ğŸ”¥ ä¿å­˜æœ€ç»ˆçš„è¯•å·åˆ°æ–‡ä»¶ï¼ˆåŒ…å«åˆ†æ•°ï¼‰
    const { mkdir } = await import('node:fs/promises')
    const outputDir = join(process.cwd(), 'output', 'quiz')
    await mkdir(outputDir, { recursive: true })

    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, 19)
    const outputFile = join(outputDir, `quiz_${timestamp}.json`)

    // è®¡ç®—åˆ†æ•°ç»Ÿè®¡
    const scoreDistribution = {
      single_choice: { perQuestion: 0, total: 0 },
      multiple_choice: { perQuestion: 0, total: 0 },
      true_false: { perQuestion: 0, total: 0 },
      totalScore: 0,
    }

    state.questions.forEach((q) => {
      const score = q.score || 0
      if (q.type === 'single_choice') {
        scoreDistribution.single_choice.total += score
        if (scoreDistribution.single_choice.perQuestion === 0)
          scoreDistribution.single_choice.perQuestion = score
      }
      else if (q.type === 'multiple_choice') {
        scoreDistribution.multiple_choice.total += score
      }
      else if (q.type === 'true_false') {
        scoreDistribution.true_false.total += score
        if (scoreDistribution.true_false.perQuestion === 0)
          scoreDistribution.true_false.perQuestion = score
      }
      scoreDistribution.totalScore += score
    })

    // è®¡ç®—å¤šé€‰é¢˜å¹³å‡åˆ†
    const multipleChoiceCount = state.questions.filter(q => q.type === 'multiple_choice').length
    if (multipleChoiceCount > 0) {
      scoreDistribution.multiple_choice.perQuestion
        = Math.round(scoreDistribution.multiple_choice.total / multipleChoiceCount)
    }

    const outputData = {
      metadata: {
        generatedAt: new Date().toISOString(),
        sourceFile: filePath,
        questionTypes,
        totalQuestions,
      },
      result: {
        questions: state.questions,
        scoreDistribution,
      },
    }

    await writeFile(outputFile, JSON.stringify(outputData, null, 2), 'utf-8')
    console.warn('ğŸ“ [é¢˜ç›®ç”Ÿæˆ] å·²ä¿å­˜åˆ°æ–‡ä»¶:', outputFile)

    return {
      ...state,
      scoreDistribution,
    }
  }
  catch (error: any) {
    throw new Error(`ç”Ÿæˆé¢˜ç›®å¤±è´¥: ${error?.message || String(error)}`)
  }
}

// ---------- æµ‹è¯• LLM è¿æ¥ ----------
export async function testLLMConnection(): Promise<{
  success: boolean
  message: string
  model?: string
  response?: string
}> {
  console.warn('ğŸ§ª [æµ‹è¯•] å¼€å§‹æµ‹è¯• LLM è¿æ¥...')

  try {
    // åˆ›å»º LLM å®ä¾‹
    const llm = makeLLM()
    console.warn('âœ… [æµ‹è¯•] LLM å®ä¾‹åˆ›å»ºæˆåŠŸ')

    // å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•é—®é¢˜
    const prompt = ChatPromptTemplate.fromMessages([
      ['system', 'ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚è¯·ç”¨ä¸€å¥è¯å›ç­”é—®é¢˜ã€‚'],
      ['human', 'è¯·è¯´"ä½ å¥½ï¼ŒLLM è¿æ¥æˆåŠŸï¼"'],
    ])

    const chain = prompt.pipe(llm)
    console.warn('ğŸ”„ [æµ‹è¯•] æ­£åœ¨å‘é€æµ‹è¯•è¯·æ±‚...')

    const result = await chain.invoke({ text: 'æµ‹è¯•' })
    const response = (result.content as string).trim()

    console.warn('âœ… [æµ‹è¯•] LLM å“åº”æˆåŠŸ!')
    console.warn('ğŸ“ [æµ‹è¯•] å“åº”å†…å®¹:', response)

    return {
      success: true,
      message: 'LLM è¿æ¥æµ‹è¯•æˆåŠŸï¼',
      model: process.env.OPENAI_API_MODEL || 'gpt-4o-mini',
      response,
    }
  }
  catch (error: any) {
    console.error('âŒ [æµ‹è¯•] LLM è¿æ¥å¤±è´¥:', error)
    console.error('é”™è¯¯è¯¦æƒ…:', {
      name: error?.name,
      message: error?.message,
      code: error?.code,
      status: error?.status,
      response: error?.response?.data,
    })

    return {
      success: false,
      message: `LLM è¿æ¥å¤±è´¥: ${error?.message || String(error)}`,
    }
  }
}
