/**
 * é…ç½®æ ¼å¼è½¬æ¢å™¨
 * å°†å‰ç«¯çš„å·¥ä½œæµé…ç½®æ ¼å¼è½¬æ¢ä¸ºåç«¯å·¥ä½œæµæ‰€éœ€çš„æ ¼å¼
 */

import type { ModelConfig, ModelInfo, WorkflowNodeConfig, WorkflowNodeType } from '../quiz/types'

/**
 * æ¨¡å‹é…ç½®ç¼“å­˜
 * é¿å…é‡å¤æŸ¥è¯¢æ•°æ®åº“
 */
const _modelConfigCache = new Map<string, ModelInfo>()
const CACHE_MAX_SIZE = 100 // æœ€å¤šç¼“å­˜100ä¸ªæ¨¡å‹é…ç½®
const CACHE_TTL = 5 * 60 * 1000 // ç¼“å­˜5åˆ†é’Ÿ

interface CacheEntry {
  data: ModelInfo
  timestamp: number
}

const modelConfigCacheWithTTL = new Map<string, CacheEntry>()

/**
 * å‰ç«¯å·¥ä½œæµé…ç½®æ ¼å¼ï¼ˆä»æ•°æ®åº“ä¸­è·å–ï¼‰
 * è¿™æ˜¯ Config.WorkflowConfig çš„å®é™…ç»“æ„
 */
interface FrontendWorkflowNodeConfig {
  displayName: string
  description: string
  modelId: string | null // display_name æ ¼å¼ï¼ˆå¦‚ "OpenAI_gpt-4"ï¼‰
  parameters: {
    temperature: number
    topP: number
    maxTokens: number
  }
  systemPrompt?: string | null
  subjectSpecific?: Record<string, string> // å­¦ç§‘ä¸“å±æ¨¡å‹ID
}

interface FrontendWorkflowConfig {
  classify: FrontendWorkflowNodeConfig
  parse_questions: FrontendWorkflowNodeConfig
  generate_questions: FrontendWorkflowNodeConfig
  review_and_score: FrontendWorkflowNodeConfig
  revise?: FrontendWorkflowNodeConfig // å‘åå…¼å®¹æ—§é…ç½®
}

/**
 * ä» modelId ä¸­æå–ä¾›åº”å•†å’Œæ¨¡å‹ä¿¡æ¯
 * æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
 * 1. display_name æ ¼å¼ï¼šProvider_ModelIDï¼ˆå¦‚ "OpenAI_gpt-4"ï¼‰
 * 2. UUID æ ¼å¼ï¼šéœ€è¦ä»æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¦‚ "ff5fae90-bc15-43d6-bb73-625f5d71cbfc"ï¼‰
 * @param modelIdOrDisplayName æ¨¡å‹IDæˆ–display_name
 * @returns ModelInfo å¯¹è±¡
 */
async function parseModelId(modelIdOrDisplayName: string | null): Promise<ModelInfo | null> {
  if (!modelIdOrDisplayName)
    return null

  // ğŸ”¥ æ£€æŸ¥ç¼“å­˜
  const cached = modelConfigCacheWithTTL.get(modelIdOrDisplayName)
  if (cached) {
    const now = Date.now()
    if (now - cached.timestamp < CACHE_TTL) {
      // console.warn('ğŸ¯ [é…ç½®ç¼“å­˜] å‘½ä¸­ç¼“å­˜:', modelIdOrDisplayName)
      return cached.data
    }
    else {
      // ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
      modelConfigCacheWithTTL.delete(modelIdOrDisplayName)
    }
  }

  // æ£€æŸ¥æ˜¯å¦ä¸º UUID æ ¼å¼ï¼ˆ8-4-4-4-12ï¼‰
  const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i
  const isUUID = uuidRegex.test(modelIdOrDisplayName)

  let modelInfo: ModelInfo | null = null

  if (isUUID) {
    // UUID æ ¼å¼ï¼šä»æ•°æ®åº“æŸ¥è¯¢æ¨¡å‹ä¿¡æ¯ï¼ˆåŒ…å«ä¾›åº”å•†å‡­è¯ï¼‰
    try {
      const { getModelWithProviderById } = await import('../db/providerService')
      const model = await getModelWithProviderById(modelIdOrDisplayName)

      if (model && model.provider) {
        console.warn('âœ… [é…ç½®è½¬æ¢] æˆåŠŸåŠ è½½æ¨¡å‹é…ç½®:', {
          modelId: model.id,
          modelName: model.model_id,
          providerName: model.provider.name,
          hasApiKey: !!model.provider.api_key,
          hasBaseUrl: !!model.provider.base_url,
        })

        modelInfo = {
          id: model.id,
          name: model.model_id,
          provider: model.provider.name.toLowerCase() as any,
          apiKey: model.provider.api_key,
          baseURL: model.provider.base_url,
        }
      }
      else {
        console.error('âŒ [é…ç½®è½¬æ¢] æ¨¡å‹æˆ–ä¾›åº”å•†ä¿¡æ¯ä¸å®Œæ•´:', {
          modelId: modelIdOrDisplayName,
          hasModel: !!model,
          hasProvider: !!model?.provider,
        })
      }
    }
    catch (error) {
      console.error('âŒ [é…ç½®è½¬æ¢] ä»æ•°æ®åº“æŸ¥è¯¢æ¨¡å‹å¤±è´¥:', error)
      return null
    }
  }
  else {
    // display_name æ ¼å¼ï¼šProvider_ModelID
    const parts = modelIdOrDisplayName.split('_')
    if (parts.length < 2)
      return null

    const provider = parts[0]
    const modelId = parts.slice(1).join('_') // å¤„ç†æ¨¡å‹IDä¸­å¯èƒ½åŒ…å«ä¸‹åˆ’çº¿çš„æƒ…å†µ

    modelInfo = {
      id: modelIdOrDisplayName,
      name: modelId,
      provider: provider.toLowerCase() as any,
      // apiKey å’Œ baseURL ç”±åç«¯ä»æ•°æ®åº“åŠ è½½
    }
  }

  // ğŸ”¥ å­˜å…¥ç¼“å­˜
  if (modelInfo) {
    modelConfigCacheWithTTL.set(modelIdOrDisplayName, {
      data: modelInfo,
      timestamp: Date.now(),
    })

    // ğŸ”¥ é™åˆ¶ç¼“å­˜å¤§å°
    if (modelConfigCacheWithTTL.size > CACHE_MAX_SIZE) {
      // åˆ é™¤æœ€æ—©çš„æ¡ç›®
      const firstKey = modelConfigCacheWithTTL.keys().next().value
      if (firstKey) {
        modelConfigCacheWithTTL.delete(firstKey)
      }
    }
  }

  return modelInfo
}

/**
 * å°†å‰ç«¯å·¥ä½œæµé…ç½®è½¬æ¢ä¸ºåç«¯æ ¼å¼
 * @param frontendConfig å‰ç«¯é…ç½®å¯¹è±¡ï¼ˆä»æ•°æ®åº“åŠ è½½ï¼‰
 * @returns åç«¯å·¥ä½œæµé…ç½®æ•°ç»„
 */
export async function convertFrontendConfigToBackend(
  frontendConfig: FrontendWorkflowConfig,
): Promise<WorkflowNodeConfig[]> {
  const result: WorkflowNodeConfig[] = []

  // éå†æ‰€æœ‰èŠ‚ç‚¹ç±»å‹
  const nodeTypes: WorkflowNodeType[] = ['classify', 'parse_questions', 'generate_questions', 'review_and_score']

  for (const nodeType of nodeTypes) {
    // ğŸ”¥ å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯ review_and_score èŠ‚ç‚¹ï¼Œä¼˜å…ˆä½¿ç”¨ review_and_scoreï¼Œå¦åˆ™ä½¿ç”¨ revise
    let frontendNode = frontendConfig[nodeType]
    if (!frontendNode && nodeType === 'review_and_score') {
      frontendNode = frontendConfig.revise
    }
    if (!frontendNode)
      continue

    // è§£ææ¨¡å‹ä¿¡æ¯ï¼ˆæ”¯æŒ UUID å’Œ display_nameï¼‰
    const modelInfo = await parseModelId(frontendNode.modelId)
    if (!modelInfo) {
      // å¦‚æœæ²¡æœ‰é…ç½®æ¨¡å‹ï¼Œè·³è¿‡ï¼ˆä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼‰
      console.warn(`âš ï¸  [é…ç½®è½¬æ¢] èŠ‚ç‚¹ ${nodeType} æ²¡æœ‰é…ç½®æ¨¡å‹æˆ–è§£æå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹`)
      continue
    }

    // æ„å»ºæ¨¡å‹é…ç½®
    const modelConfig: ModelConfig = {
      temperature: frontendNode.parameters.temperature,
      top_p: frontendNode.parameters.topP,
      max_tokens: frontendNode.parameters.maxTokens,
    }

    // æ„å»ºåç«¯èŠ‚ç‚¹é…ç½®
    const backendNode: WorkflowNodeConfig = {
      nodeType,
      modelInfo,
      config: modelConfig,
      systemPrompt: frontendNode.systemPrompt || undefined,
    }

    // å¤„ç†å­¦ç§‘ä¸“å±æ¨¡å‹ï¼ˆå¦‚æœæœ‰ï¼‰
    if (frontendNode.subjectSpecific) {
      const subjectSpecific: Partial<Record<string, ModelInfo>> = {}
      for (const [subject, modelIdOrDisplayName] of Object.entries(frontendNode.subjectSpecific)) {
        const subjectModelInfo = await parseModelId(modelIdOrDisplayName)
        if (subjectModelInfo) {
          subjectSpecific[subject] = subjectModelInfo
        }
      }
      if (Object.keys(subjectSpecific).length > 0) {
        backendNode.subjectSpecific = subjectSpecific as any
      }
    }

    result.push(backendNode)
  }

  return result
}

/**
 * æµ‹è¯•å‡½æ•°ï¼šéªŒè¯è½¬æ¢é€»è¾‘
 */
export function testConverter() {
  const frontendConfig: FrontendWorkflowConfig = {
    classify: {
      displayName: 'é¢˜ç›®åˆ†ç±»',
      description: 'è¯†åˆ«é¢˜ç›®æ‰€å±å­¦ç§‘',
      modelId: 'OpenAI_gpt-4o-mini',
      parameters: {
        temperature: 0.3,
        topP: 0.8,
        maxTokens: 2048,
      },
      systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨...',
    },
    parse_questions: {
      displayName: 'é¢˜ç›®è§£æ',
      description: 'æå–é¢˜ç›®ä¿¡æ¯',
      modelId: 'OpenAI_gpt-4',
      parameters: {
        temperature: 0.5,
        topP: 0.9,
        maxTokens: 4096,
      },
    },
    generate_questions: {
      displayName: 'é¢˜ç›®ç”Ÿæˆ',
      description: 'ç”Ÿæˆç»ƒä¹ é¢˜',
      modelId: 'OpenAI_gpt-4',
      parameters: {
        temperature: 0.8,
        topP: 0.95,
        maxTokens: 8192,
      },
      systemPrompt: 'ä½ æ˜¯å‡ºé¢˜åŠ©æ‰‹...',
      subjectSpecific: {
        math: 'OpenAI_gpt-4',
        physics: 'DeepSeek_deepseek-chat',
      },
    },
    revise: {
      displayName: 'ç»“æœå®¡æ ¸',
      description: 'æ£€æŸ¥é¢˜ç›®è´¨é‡',
      modelId: null,
      parameters: {
        temperature: 0.3,
        topP: 0.8,
        maxTokens: 4096,
      },
    },
  }

  const backendConfig = convertFrontendConfigToBackend(frontendConfig)
  console.warn('è½¬æ¢ç»“æœ:', JSON.stringify(backendConfig, null, 2))
  return backendConfig
}
